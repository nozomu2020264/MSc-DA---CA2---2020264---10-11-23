{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7cdc59e",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7ce788d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import sparknlp\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CassandraIntegration\") \\\n",
    "    .config(\"spark.cassandra.connection.host\", \"127.0.0.1\") \\\n",
    "    .config(\"spark.cassandra.connection.port\", \"9042\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a64ae8e4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to import dataset: 11.111948728561401 seconds.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "df = spark.read \\\n",
    "    .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "    .options(table=\"tweets\", keyspace=\"tweets\") \\\n",
    "    .load()\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Time taken to import dataset: {end_time - start_time} seconds.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2aeaf9c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+--------+----------+--------------------+---------------+\n",
      "|sequence|              dates|    flag|       ids|                text|           user|\n",
      "+--------+-------------------+--------+----------+--------------------+---------------+\n",
      "|  432373|2009-06-07 08:02:44|NO_QUERY|2064736572|\"uh...2morrow vac...|       Nessa128|\n",
      "|  780948|2009-06-25 01:20:18|NO_QUERY|2323301700|\"@toeekneee why??...|    DaniFeldman|\n",
      "|   83512|2009-05-10 02:16:40|NO_QUERY|1753364328|\"twisted my ankle...|     saysjessie|\n",
      "|   62129|2009-05-03 08:24:01|NO_QUERY|1686978943|\"Heading to Magic...| foldinglaundry|\n",
      "|  760443|2009-06-23 10:35:54|NO_QUERY|2296772072|\"@willpug i would...|        Mizannn|\n",
      "|   73918|2009-05-04 04:36:59|NO_QUERY|1694661852|\"@Void_Shanghai i...|        duzkiez|\n",
      "|  226995|2009-05-30 23:07:22|NO_QUERY|1977969653|\"I feel sad. I wa...|          BAMFx|\n",
      "|  817205|2009-04-18 10:42:00|NO_QUERY|1551682133|\"@mya152 Marks he...|       giuli272|\n",
      "|  104285|2009-05-16 20:34:00|NO_QUERY|1822433004|\"Concerts hurt my...| DannyGeezy1819|\n",
      "| 1253567|2009-06-01 16:55:39|NO_QUERY|1996918208|\"Watching the Bac...|   jackiedamien|\n",
      "| 1004090|2009-05-22 02:02:29|NO_QUERY|1880382023|\"Andy is a trendi...|  SunshineeKiid|\n",
      "|  762453|2009-06-23 11:58:13|NO_QUERY|2297865230|\"Was a great morn...|       AbbeyXIX|\n",
      "| 1329936|2009-06-03 06:24:28|NO_QUERY|2015739257|\"I so passed Hist...|     lydianiamh|\n",
      "|  346723|2009-06-03 07:26:47|NO_QUERY|2016249960|\"Ah a good work o...|     luvyajanet|\n",
      "|  264812|2009-05-31 20:36:17|NO_QUERY|1986976886|\"McFly on Panico ...|giovannarinaldi|\n",
      "| 1596798|2009-06-16 08:31:21|NO_QUERY|2192768882|\"@churchpunkmom t...|CouchSurfingOri|\n",
      "|  692170|2009-06-20 07:14:51|NO_QUERY|2252429232|\"Throat is on FiR...|      MissJodie|\n",
      "|  728714|2009-06-21 01:03:09|NO_QUERY|2263140879|\"@nabil_ismfof Yo...|    JordanVelle|\n",
      "|  851960|2009-04-20 23:45:50|NO_QUERY|1572845258|\"ok i want to see...|  chelseasymone|\n",
      "|  130346|2009-05-18 06:38:56|NO_QUERY|1835294007|\"8 working days a...|    poodlealice|\n",
      "+--------+-------------------+--------+----------+--------------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# df = df.sample(False, 0.1)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70445571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sequence: integer (nullable = false)\n",
      " |-- dates: timestamp (nullable = true)\n",
      " |-- flag: string (nullable = true)\n",
      " |-- ids: long (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- user: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7da9fae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+--------+----------+--------------------+---------------+\n",
      "|sequence|              dates|    flag|       ids|                text|           user|\n",
      "+--------+-------------------+--------+----------+--------------------+---------------+\n",
      "|       0|2009-04-06 23:19:45|NO_QUERY|1467810369|\"@switchfoot http...|_TheSpecialOne_|\n",
      "|       1|2009-04-06 23:19:49|NO_QUERY|1467810672|\"is upset that he...|  scotthamilton|\n",
      "|       2|2009-04-06 23:19:53|NO_QUERY|1467810917|\"@Kenichan I dive...|       mattycus|\n",
      "|       3|2009-04-06 23:19:57|NO_QUERY|1467811184|\"my whole body fe...|        ElleCTF|\n",
      "|       4|2009-04-06 23:19:57|NO_QUERY|1467811193|\"@nationwideclass...|         Karoli|\n",
      "|       5|2009-04-06 23:20:00|NO_QUERY|1467811372|\"@Kwesidei not th...|       joy_wolf|\n",
      "|       6|2009-04-06 23:20:03|NO_QUERY|1467811592|       \"Need a hug \"|        mybirch|\n",
      "|       7|2009-04-06 23:20:03|NO_QUERY|1467811594|\"@LOLTrish hey  l...|           coZZ|\n",
      "|       8|2009-04-06 23:20:05|NO_QUERY|1467811795|\"@Tatiana_K nope ...|2Hood4Hollywood|\n",
      "|       9|2009-04-06 23:20:09|NO_QUERY|1467812025|\"@twittera que me...|        mimismo|\n",
      "|      10|2009-04-06 23:20:16|NO_QUERY|1467812416|\"spring break in ...| erinx3leannexo|\n",
      "|      11|2009-04-06 23:20:17|NO_QUERY|1467812579|\"I just re-pierce...|   pardonlauren|\n",
      "|      12|2009-04-06 23:20:19|NO_QUERY|1467812723|\"@caregiving I co...|           TLeC|\n",
      "|      13|2009-04-06 23:20:19|NO_QUERY|1467812771|\"@octolinz16 It i...|robrobbierobert|\n",
      "|      14|2009-04-06 23:20:20|NO_QUERY|1467812784|\"@smarrison i wou...|    bayofwolves|\n",
      "|      15|2009-04-06 23:20:20|NO_QUERY|1467812799|\"@iamjazzyfizzle ...|     HairByJess|\n",
      "|      16|2009-04-06 23:20:22|NO_QUERY|1467812964|\"Hollis' death sc...| lovesongwriter|\n",
      "|      17|2009-04-06 23:20:25|NO_QUERY|1467813137|\"about to file ta...|       armotley|\n",
      "|      18|2009-04-06 23:20:31|NO_QUERY|1467813579|\"@LettyA ahh ive ...|     starkissed|\n",
      "|      19|2009-04-06 23:20:34|NO_QUERY|1467813782|\"@FakerPattyPattz...|      gi_gi_bee|\n",
      "+--------+-------------------+--------+----------+--------------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Time taken to perform: 48.1657133102417 seconds.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "start_time = time.time()\n",
    "\n",
    "df = df.orderBy(col(\"sequence\"))\n",
    "df.show()\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Time taken to perform: {end_time - start_time} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ad71693",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:=================================================>         (5 + 1) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+\n",
      "|              dates|                text|\n",
      "+-------------------+--------------------+\n",
      "|2009-04-06 23:19:45|\"@switchfoot http...|\n",
      "|2009-04-06 23:19:49|\"is upset that he...|\n",
      "|2009-04-06 23:19:53|\"@Kenichan I dive...|\n",
      "|2009-04-06 23:19:57|\"my whole body fe...|\n",
      "|2009-04-06 23:19:57|\"@nationwideclass...|\n",
      "|2009-04-06 23:20:00|\"@Kwesidei not th...|\n",
      "|2009-04-06 23:20:03|       \"Need a hug \"|\n",
      "|2009-04-06 23:20:03|\"@LOLTrish hey  l...|\n",
      "|2009-04-06 23:20:05|\"@Tatiana_K nope ...|\n",
      "|2009-04-06 23:20:09|\"@twittera que me...|\n",
      "|2009-04-06 23:20:16|\"spring break in ...|\n",
      "|2009-04-06 23:20:17|\"I just re-pierce...|\n",
      "|2009-04-06 23:20:19|\"@caregiving I co...|\n",
      "|2009-04-06 23:20:19|\"@octolinz16 It i...|\n",
      "|2009-04-06 23:20:20|\"@smarrison i wou...|\n",
      "|2009-04-06 23:20:20|\"@iamjazzyfizzle ...|\n",
      "|2009-04-06 23:20:22|\"Hollis' death sc...|\n",
      "|2009-04-06 23:20:25|\"about to file ta...|\n",
      "|2009-04-06 23:20:31|\"@LettyA ahh ive ...|\n",
      "|2009-04-06 23:20:34|\"@FakerPattyPattz...|\n",
      "+-------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Time taken to perform: 37.881097078323364 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "df = df.drop(\"_id\", \"flag\", \"ids\", \"user\", \"date\", \"sequence\")\n",
    "df.show()\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Time taken to perform: {end_time - start_time} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbe9f710",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:=================================================>         (5 + 1) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+\n",
      "|dates|text|\n",
      "+-----+----+\n",
      "|    0|   0|\n",
      "+-----+----+\n",
      "\n",
      "Time taken to perform: 38.47193002700806 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "start_time = time.time()\n",
    "# Counting missing values for each column\n",
    "missing_counts = df.select([F.count(F.when(F.isnull(c), c)).alias(c) for c in df.columns])\n",
    "\n",
    "missing_counts.show()\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Time taken to perform: {end_time - start_time} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5814ed6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 12:================================================>         (5 + 1) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries with URLs: 70068\n",
      "Number of entries with HTML tags: 0\n",
      "Number of entries with mentions: 738491\n",
      "Time taken to perform: 110.67055130004883 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Count entries with URLs\n",
    "url_count = df.filter(F.col(\"text\").rlike(\"http(s)?://([\\\\w-]+\\\\.)+[\\\\w-]+(/[\\\\w- ./?%&=]*)?\")).count()\n",
    "\n",
    "# Count entries with HTML tags\n",
    "html_tags_count = df.filter(F.col(\"text\").rlike(\"<[^>]+>\")).count()\n",
    "\n",
    "# Count entries with mentions (@username)\n",
    "mentions_count = df.filter(F.col(\"text\").rlike(\"@\\\\w+\")).count()\n",
    "\n",
    "print(f\"Number of entries with URLs: {url_count}\")\n",
    "print(f\"Number of entries with HTML tags: {html_tags_count}\")\n",
    "print(f\"Number of entries with mentions: {mentions_count}\")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Time taken to perform: {end_time - start_time} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ae28d1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 15:================================================>         (5 + 1) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------------------+\n",
      "|text                                                                                                            |\n",
      "+----------------------------------------------------------------------------------------------------------------+\n",
      "|\"  - a11, that's a bummer.  you shoulda got david carr of third day to do it. ;d\"                               |\n",
      "|\"is upset that he can't update his facebook by texting it11 and might cry as a result  school today also. blah!\"|\n",
      "|\" i dived many times for the ball. managed to save %  the rest go out of bounds\"                                |\n",
      "|\"my whole body feels itchy and like its on fire \"                                                               |\n",
      "|\" no, it's not behaving at all. i'm mad. why am i here? because i can't see you all over there. \"               |\n",
      "|\" not the whole crew \"                                                                                          |\n",
      "|\"need a hug \"                                                                                                   |\n",
      "|\" hey  long time no see! yes.. rains a bit ,only a bit  lol , i'm fine thanks , how's you ?\"                    |\n",
      "|\" nope they didn't have it \"                                                                                    |\n",
      "|\" que me muera ? \"                                                                                              |\n",
      "|\"spring break in plain city11 it's snowing \"                                                                    |\n",
      "|\"i just re-pierced my ears \"                                                                                    |\n",
      "|\" i couldn't bear to watch it.  and i thought the ua loss was embarrassing . . . . .\"                           |\n",
      "|\" it it counts, idk why i did either. you never talk to me anymore \"                                            |\n",
      "|\" i would've been the first, but i didn't have a gun.11not really though, zac snyder's just a doucheclown.\"     |\n",
      "|\" i wish i got to watch it with you!! i miss you and11how was the premiere?!\"                                   |\n",
      "|\"hollis' death scene will hurt me severely to watch on film  wry is directors cut not out now?\"                 |\n",
      "|\"about to file taxes \"                                                                                          |\n",
      "|\" ahh ive always wanted to see rent  love the soundtrack!!\"                                                     |\n",
      "|\" oh dear. were you drinking out of the forgotten table drinks? \"                                               |\n",
      "+----------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Time taken to perform: 32.21481275558472 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "from pyspark.sql.functions import lower\n",
    "start_time = time.time()\n",
    "\n",
    "# Remove URLs\n",
    "df = df.withColumn(\"text\", F.regexp_replace(F.col(\"text\"), \"http(s)?://[^\\\\s]+\", \"\"))\n",
    "\n",
    "# Remove HTML tags\n",
    "df = df.withColumn(\"text\", F.regexp_replace(F.col(\"text\"), \"<[^>]+>\", \"\"))\n",
    "\n",
    "# Remove mentions (i.e., @username)\n",
    "df = df.withColumn(\"text\", F.regexp_replace(F.col(\"text\"), \"@\\\\w+\", \"\"))\n",
    "\n",
    "# Convert to lowercase\n",
    "df = df.withColumn('text', lower(df['text']))\n",
    "\n",
    "# Remove numbers from the \"text\" column\n",
    "df = df.withColumn('text', regexp_replace(df['text'], r'\\d+', ''))\n",
    "\n",
    "# Reduce excessive characters (more than two of the same in a row)\n",
    "df = df.withColumn('text', regexp_replace('text', r'(.)\\1{2,}', r'\\1\\1'))\n",
    "\n",
    "df.select(\"text\").show(truncate=False)\n",
    "end_time = time.time()\n",
    "print(f\"Time taken to perform: {end_time - start_time} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec971895",
   "metadata": {},
   "source": [
    "- The remaining characters will be erased by removing punctuations in the following steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36e7c4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import contractions\n",
    "import json\n",
    "\n",
    "# Extract contractions and save to a JSON file\n",
    "with open(\"contractions.json\", \"w\") as f:\n",
    "    json.dump(contractions.contractions_dict, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f01e34cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------------------------------+\n",
      "|text                                                                                                             |\n",
      "+-----------------------------------------------------------------------------------------------------------------+\n",
      "|\"  - a11, that is a bummer.  you shoulda got david carr of third day to do it. ;d\"                               |\n",
      "|\"is upset that he cannot update his facebook by texting it11 and might cry as a result  school today also. blah!\"|\n",
      "|\" i dived many times for the ball. managed to save %  the rest go out of bounds\"                                 |\n",
      "|\"my whole body feels itchy and like its on fire \"                                                                |\n",
      "|\" no, it is not behaving at all. I am mad. why am i here? because i cannot see you all over there. \"             |\n",
      "|\" not the whole crew \"                                                                                           |\n",
      "|\"need a hug \"                                                                                                    |\n",
      "|\" hey  long time no see! yes.. rains a bit ,only a bit  lol , I am fine thanks , how is you ?\"                   |\n",
      "|\" nope they did not have it \"                                                                                    |\n",
      "|\" que me muera ? \"                                                                                               |\n",
      "|\"spring break in plain city11 it is snowing \"                                                                    |\n",
      "|\"i just re-pierced my ears \"                                                                                     |\n",
      "|\" i could not bear to watch it.  and i thought the ua loss was embarrassing . . . . .\"                           |\n",
      "|\" it it counts, idk why i did either. you never talk to me anymore \"                                             |\n",
      "|\" i would have been the first, but i did not have a gun.11not really though, zac snyder's just a doucheclown.\"   |\n",
      "|\" i wish i got to watch it with you!! i miss you and11how was the premiere?!\"                                    |\n",
      "|\"hollis' death scene will hurt me severely to watch on film  wry is directors cut not out now?\"                  |\n",
      "|\"about to file taxes \"                                                                                           |\n",
      "|\" ahh ive always wanted to see rent  love the soundtrack!!\"                                                      |\n",
      "|\" oh dear. were you drinking out of the forgotten table drinks? \"                                                |\n",
      "+-----------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Time taken to perform: 53.487390756607056 seconds.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, regexp_replace\n",
    "import re\n",
    "start_time = time.time()\n",
    "# Load contractions from the JSON file\n",
    "with open(\"contractions.json\", \"r\") as f:\n",
    "    contractions_dict = json.load(f)\n",
    "\n",
    "for contraction, expansion in contractions_dict.items():\n",
    "    # Ensure regex special characters are escaped\n",
    "    contraction_regex = re.escape(contraction)\n",
    "    \n",
    "    # Use word boundaries and case-insensitive match\n",
    "    pattern = r\"(?i)\\b{}\\b\".format(contraction_regex)\n",
    "    \n",
    "    df = df.withColumn(\"text\", regexp_replace(col(\"text\"), pattern, expansion))\n",
    "    \n",
    "df.select(\"text\").show(truncate=False)\n",
    "end_time = time.time()\n",
    "print(f\"Time taken to perform: {end_time - start_time} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "189f4f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------+\n",
      "|text                                                                                                         |\n",
      "+-------------------------------------------------------------------------------------------------------------+\n",
      "|   a11 that is a bummer  you shoulda got david carr of third day to do it d                                  |\n",
      "|is upset that he cannot update his facebook by texting it11 and might cry as a result  school today also blah|\n",
      "| i dived many times for the ball managed to save   the rest go out of bounds                                 |\n",
      "|my whole body feels itchy and like its on fire                                                               |\n",
      "| no it is not behaving at all I am mad why am i here because i cannot see you all over there                 |\n",
      "| not the whole crew                                                                                          |\n",
      "|need a hug                                                                                                   |\n",
      "| hey  long time no see yes rains a bit only a bit  lol  I am fine thanks  how is you                         |\n",
      "| nope they did not have it                                                                                   |\n",
      "| que me muera                                                                                                |\n",
      "|spring break in plain city11 it is snowing                                                                   |\n",
      "|i just repierced my ears                                                                                     |\n",
      "| i could not bear to watch it  and i thought the ua loss was embarrassing                                    |\n",
      "| it it counts idk why i did either you never talk to me anymore                                              |\n",
      "| i would have been the first but i did not have a gun11not really though zac snyders just a doucheclown      |\n",
      "| i wish i got to watch it with you i miss you and11how was the premiere                                      |\n",
      "|hollis death scene will hurt me severely to watch on film  wry is directors cut not out now                  |\n",
      "|about to file taxes                                                                                          |\n",
      "| ahh ive always wanted to see rent  love the soundtrack                                                      |\n",
      "| oh dear were you drinking out of the forgotten table drinks                                                 |\n",
      "+-------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove punctuation\n",
    "df = df.withColumn('text', regexp_replace(df['text'], r\"[^\\w\\s]\", \"\"))\n",
    "df.select(\"text\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14556a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spellcheck_norvig download started this may take some time.\n",
      "Approximate size to download 4.2 MB\n",
      "[ / ]spellcheck_norvig download started this may take some time.\n",
      "Approximate size to download 4.2 MB\n",
      "[ | ]Download done! Loading the resource.\n",
      "[ / ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 19:==============>                                          (3 + 3) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[ — ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 19:============================>                            (6 + 2) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[ \\ ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK!]\n",
      "glove_100d download started this may take some time.\n",
      "Approximate size to download 145.3 MB\n",
      "[ / ]glove_100d download started this may take some time.\n",
      "Approximate size to download 145.3 MB\n",
      "[ / ]Download done! Loading the resource.\n",
      "[OK!]\n",
      "sentimentdl_glove_imdb download started this may take some time.\n",
      "Approximate size to download 8.7 MB\n",
      "[ — ]sentimentdl_glove_imdb download started this may take some time.\n",
      "Approximate size to download 8.7 MB\n",
      "[ | ]Download done! Loading the resource.\n",
      "[ / ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \\ ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-03 23:04:17.513512: I external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "from sparknlp.annotator import *\n",
    "from sparknlp.base import *\n",
    "\n",
    "document_assembler = DocumentAssembler() \\\n",
    "    .setInputCol(\"text\") \\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "sentence_detector = SentenceDetector() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"sentence\")\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols([\"sentence\"]) \\\n",
    "    .setOutputCol(\"token\")\n",
    "\n",
    "# 5. Context-aware Spell Checker\n",
    "spellChecker = NorvigSweetingModel.pretrained() \\\n",
    "    .setInputCols([\"token\"]) \\\n",
    "    .setOutputCol(\"corrected\")\n",
    "\n",
    "# Use GloVe embeddings (or any other word embeddings that you prefer)\n",
    "embeddings = WordEmbeddingsModel.pretrained('glove_100d') \\\n",
    "    .setInputCols([\"sentence\", \"corrected\"]) \\\n",
    "    .setOutputCol(\"embeddings\")\n",
    "\n",
    "# Convert word embeddings to sentence embeddings\n",
    "sentence_embeddings = SentenceEmbeddings() \\\n",
    "    .setInputCols([\"sentence\", \"embeddings\"]) \\\n",
    "    .setOutputCol(\"sentence_embeddings\") \\\n",
    "    .setPoolingStrategy(\"AVERAGE\")\n",
    "\n",
    "# Use the SentimentDLModel compatible with GloVe embeddings\n",
    "sentiment_detector = SentimentDLModel.pretrained('sentimentdl_glove_imdb', 'en') \\\n",
    "    .setInputCols([\"sentence_embeddings\"]) \\\n",
    "    .setOutputCol(\"sentiment\") \\\n",
    "    .setThreshold(0.7) \\\n",
    "\n",
    "# Finisher to convert annotations to DataFrame columns\n",
    "finisher = Finisher() \\\n",
    "    .setInputCols([\"sentiment\"]) \\\n",
    "    .setIncludeMetadata(True)  # Set to True to include metadata (which should include scores)\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "    document_assembler,\n",
    "    sentence_detector,\n",
    "    tokenizer,\n",
    "    spellChecker,\n",
    "    embeddings,\n",
    "    sentence_embeddings,\n",
    "    sentiment_detector,\n",
    "    finisher\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b00e194",
   "metadata": {},
   "source": [
    "from sparknlp.annotator import *\n",
    "from sparknlp.base import *\n",
    "document_assembler = DocumentAssembler() \\\n",
    "    .setInputCol(\"text\") \\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "sentence_detector = SentenceDetector() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"sentence\")\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"token\")\n",
    "\n",
    "# Load TensorFlow Hub Universal Sentence Encoder embeddings\n",
    "use_embeddings = UniversalSentenceEncoder.pretrained(name=\"tfhub_use\", lang=\"en\") \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"sentence_embeddings\")\n",
    "\n",
    "# Assuming 'sentimentdl_use_twitter' expects USE embeddings, which should be compatible with the lite version\n",
    "sentiment_detector = SentimentDLModel.pretrained('sentimentdl_use_twitter', 'en') \\\n",
    "    .setInputCols([\"sentence_embeddings\"]) \\\n",
    "    .setOutputCol(\"sentiment\") \\\n",
    "    .setThreshold(0.7)\n",
    "\n",
    "finisher = Finisher() \\\n",
    "    .setInputCols([\"sentiment\"]) \\\n",
    "    .setIncludeMetadata(True)  # Set to True to include metadata (which should include scores)\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "    document_assembler,\n",
    "    sentence_detector,\n",
    "    tokenizer,\n",
    "    use_embeddings,\n",
    "    sentiment_detector,\n",
    "    finisher\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "293e15a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------+------------------+-------------------------------------------------------+\n",
      "|text                                                                                                         |finished_sentiment|finished_sentiment_metadata                            |\n",
      "+-------------------------------------------------------------------------------------------------------------+------------------+-------------------------------------------------------+\n",
      "|   a11 that is a bummer  you shoulda got david carr of third day to do it d                                  |[pos]             |[{sentence, 0}, {pos, 0.97958183}, {neg, 0.020418175}] |\n",
      "|is upset that he cannot update his facebook by texting it11 and might cry as a result  school today also blah|[neg]             |[{sentence, 0}, {pos, 0.18526705}, {neg, 0.81473297}]  |\n",
      "| i dived many times for the ball managed to save   the rest go out of bounds                                 |[neg]             |[{sentence, 0}, {pos, 3.6196804E-4}, {neg, 0.99963796}]|\n",
      "|my whole body feels itchy and like its on fire                                                               |[pos]             |[{sentence, 0}, {pos, 0.99901366}, {neg, 9.863286E-4}] |\n",
      "| no it is not behaving at all I am mad why am i here because i cannot see you all over there                 |[neg]             |[{sentence, 0}, {pos, 0.0024271114}, {neg, 0.99757284}]|\n",
      "| not the whole crew                                                                                          |[neg]             |[{sentence, 0}, {pos, 8.40983E-5}, {neg, 0.99991584}]  |\n",
      "|need a hug                                                                                                   |[pos]             |[{sentence, 0}, {pos, 0.99998593}, {neg, 1.4074805E-5}]|\n",
      "| hey  long time no see yes rains a bit only a bit  lol  I am fine thanks  how is you                         |[neg]             |[{sentence, 0}, {pos, 0.09902905}, {neg, 0.90097094}]  |\n",
      "| nope they did not have it                                                                                   |[neg]             |[{sentence, 0}, {pos, 1.108642E-6}, {neg, 0.9999989}]  |\n",
      "| que me muera                                                                                                |[neg]             |[{sentence, 0}, {pos, 4.155031E-8}, {neg, 1.0}]        |\n",
      "|spring break in plain city11 it is snowing                                                                   |[neg]             |[{sentence, 0}, {pos, 1.9618229E-4}, {neg, 0.99980384}]|\n",
      "|i just repierced my ears                                                                                     |[neg]             |[{sentence, 0}, {pos, 0.16082759}, {neg, 0.83917236}]  |\n",
      "| i could not bear to watch it  and i thought the ua loss was embarrassing                                    |[neg]             |[{sentence, 0}, {pos, 1.276122E-4}, {neg, 0.9998723}]  |\n",
      "| it it counts idk why i did either you never talk to me anymore                                              |[neg]             |[{sentence, 0}, {pos, 4.7921183E-4}, {neg, 0.9995208}] |\n",
      "| i would have been the first but i did not have a gun11not really though zac snyders just a doucheclown      |[neg]             |[{sentence, 0}, {pos, 0.029194182}, {neg, 0.9708058}]  |\n",
      "| i wish i got to watch it with you i miss you and11how was the premiere                                      |[pos]             |[{sentence, 0}, {pos, 0.99999905}, {neg, 9.167321E-7}] |\n",
      "|hollis death scene will hurt me severely to watch on film  wry is directors cut not out now                  |[neg]             |[{sentence, 0}, {pos, 3.8864592E-4}, {neg, 0.99961144}]|\n",
      "|about to file taxes                                                                                          |[neg]             |[{sentence, 0}, {pos, 1.4169399E-6}, {neg, 0.99999857}]|\n",
      "| ahh ive always wanted to see rent  love the soundtrack                                                      |[pos]             |[{sentence, 0}, {pos, 0.9999982}, {neg, 1.7321858E-6}] |\n",
      "| oh dear were you drinking out of the forgotten table drinks                                                 |[neg]             |[{sentence, 0}, {pos, 4.6565762E-4}, {neg, 0.99953437}]|\n",
      "+-------------------------------------------------------------------------------------------------------------+------------------+-------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply the pipeline to the DataFrame\n",
    "pipeline_model = pipeline.fit(df)\n",
    "df = pipeline_model.transform(df)\n",
    "\n",
    "# Display the results\n",
    "df.select(\"text\",\"finished_sentiment\", \"finished_sentiment_metadata\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9443c2c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+------------------+-------------------+\n",
      "|              dates|                text|finished_sentiment|    sentiment_score|\n",
      "+-------------------+--------------------+------------------+-------------------+\n",
      "|2009-04-06 23:19:45|   a11 that is a ...|             [pos]|        0.959163655|\n",
      "|2009-04-06 23:19:49|is upset that he ...|             [neg]|        -0.62946592|\n",
      "|2009-04-06 23:19:53| i dived many tim...|             [neg]|     -0.99927599196|\n",
      "|2009-04-06 23:19:57|my whole body fee...|             [pos]|       0.9980273314|\n",
      "|2009-04-06 23:19:57| no it is not beh...|             [neg]|-0.9951457286000001|\n",
      "|2009-04-06 23:20:00| not the whole crew |             [neg]|      -0.9998317417|\n",
      "|2009-04-06 23:20:03|         need a hug |             [pos]|     0.999971855195|\n",
      "|2009-04-06 23:20:03| hey  long time n...|             [neg]|        -0.80194189|\n",
      "|2009-04-06 23:20:05| nope they did no...|             [neg]|    -0.999997791358|\n",
      "|2009-04-06 23:20:09|      que me muera  |             [neg]|  -0.99999995844969|\n",
      "|2009-04-06 23:20:16|spring break in p...|             [neg]|     -0.99960765771|\n",
      "|2009-04-06 23:20:17|i just repierced ...|             [neg]|-0.6783447699999999|\n",
      "|2009-04-06 23:20:19| i could not bear...|             [neg]|      -0.9997446878|\n",
      "|2009-04-06 23:20:19| it it counts idk...|             [neg]|     -0.99904158817|\n",
      "|2009-04-06 23:20:20| i would have bee...|             [neg]|       -0.941611618|\n",
      "|2009-04-06 23:20:20| i wish i got to ...|             [pos]|    0.9999981332679|\n",
      "|2009-04-06 23:20:22|hollis death scen...|             [neg]|     -0.99741009408|\n",
      "|2009-04-06 23:20:25|about to file taxes |             [neg]|-0.9999971530601001|\n",
      "|2009-04-06 23:20:31| ahh ive always w...|             [pos]| 0.9999964678141999|\n",
      "|2009-04-06 23:20:34| oh dear were you...|             [neg]|-0.9990687123800001|\n",
      "+-------------------+--------------------+------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, expr\n",
    "\n",
    "# Extract positive and negative scores\n",
    "df = df.withColumn(\"positive_score\", expr(\"filter(finished_sentiment_metadata, x -> x._1 == 'pos')[0]._2\"))\n",
    "df = df.withColumn(\"negative_score\", expr(\"filter(finished_sentiment_metadata, x -> x._1 == 'neg')[0]._2\"))\n",
    "\n",
    "# Calculate sentiment score by subtracting the negative score from the positive score\n",
    "df = df.withColumn(\"sentiment_score\", col(\"positive_score\") - col(\"negative_score\"))\n",
    "\n",
    "# Show the resulting DataFrame with sentiment score\n",
    "df.select(\"dates\", \"text\", \"finished_sentiment\", \"sentiment_score\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cfac1111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dates: timestamp (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- finished_sentiment: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- finished_sentiment_metadata: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- _1: string (nullable = true)\n",
      " |    |    |-- _2: string (nullable = true)\n",
      " |-- positive_score: string (nullable = true)\n",
      " |-- negative_score: string (nullable = true)\n",
      " |-- sentiment_score: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "37aaf6e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 27:================================================>         (5 + 1) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (1600000, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "num_rows = df.count() # Counting the number of rows.\n",
    "num_columns =len(df.columns) # Length of columns.\n",
    "print(f\"Shape: ({num_rows}, {num_columns})\") # Prints the shape of the dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
