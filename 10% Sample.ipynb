{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7826fca",
   "metadata": {},
   "source": [
    "# import, cleaning and preping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59341c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import sparknlp\n",
    "\n",
    "# Initialize Spark session with all configurations\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"myApp\") \\\n",
    "    .config(\"spark.mongodb.read.connection.uri\", \"mongodb://127.0.0.1/twitterData.tweets\") \\\n",
    "    .config(\"spark.mongodb.write.connection.uri\", \"mongodb://127.0.0.1/twitterData.tweets\") \\\n",
    "    .config(\"spark.jars\", \"/home/hduser/Desktop/mongo-spark-connector_2.12-10.2.0.jar\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:5.1.4\")\\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41882623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define ONNX parameters\n",
    "#onnx_params = {\n",
    "#    \"spark.jsl.settings.onnx.gpuDeviceId\": \"0\",\n",
    "#    \"spark.jsl.settings.onnx.intraOpNumThreads\": \"5\",\n",
    "#    \"spark.jsl.settings.onnx.optimizationLevel\": \"BASIC_OPT\",\n",
    "#    \"spark.jsl.settings.onnx.executionMode\": \"SEQUENTIAL\"\n",
    "\n",
    "#    .config(\"spark.jsl.settings.onnx.gpuDeviceId\", onnx_params[\"spark.jsl.settings.onnx.gpuDeviceId\"]) \\\n",
    "#    .config(\"spark.jsl.settings.onnx.intraOpNumThreads\", onnx_params[\"spark.jsl.settings.onnx.intraOpNumThreads\"]) \\\n",
    "#    .config(\"spark.jsl.settings.onnx.optimizationLevel\", onnx_params[\"spark.jsl.settings.onnx.optimizationLevel\"]) \\\n",
    "#    .config(\"spark.jsl.settings.onnx.executionMode\", onnx_params[\"spark.jsl.settings.onnx.executionMode\"]) \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d91e26b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"mongodb\") \\\n",
    "    .option(\"spark.mongodb.input.uri\", \"mongodb://localhost:27017/twitterData.tweets\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d0538d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(False, 0.01)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84264e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 0) / 1]\r",
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------+----------+--------+--------------------+---------------+\n",
      "|                 _id|                date|    flag|       ids|sequence|                text|           user|\n",
      "+--------------------+--------------------+--------+----------+--------+--------------------+---------------+\n",
      "|65325dcdb4cad9b16...|Mon Apr 06 22:28:...|NO_QUERY|1467843734|     146|my nokia 1110 died..|      chatpataa|\n",
      "|65325dcdb4cad9b16...|Mon Apr 06 22:36:...|NO_QUERY|1467873980|     246|missed Brent at p...|     gregcronin|\n",
      "|65325dcdb4cad9b16...|Mon Apr 06 22:40:...|NO_QUERY|1467890212|     301|Mraow, I feel lik...|   MorganWillis|\n",
      "|65325dcdb4cad9b16...|Mon Apr 06 22:57:...|NO_QUERY|1467951422|     561|sad that the 'fee...|        felloff|\n",
      "|65325dcdb4cad9b16...|Mon Apr 06 23:00:...|NO_QUERY|1467962336|     592|my heart is broke...|          umfoo|\n",
      "|65325dcdb4cad9b16...|Mon Apr 06 23:01:...|NO_QUERY|1467962671|     595|I can't take this...|  Bi0hazard2886|\n",
      "|65325dcdb4cad9b16...|Mon Apr 06 23:06:...|NO_QUERY|1467981027|     663|im sooo sad right...|  maTERIALfemme|\n",
      "|65325dcdb4cad9b16...|Mon Apr 06 23:08:...|NO_QUERY|1467989188|     714|@stephenkruiser I...|     littlefoxy|\n",
      "|65325dcdb4cad9b16...|Mon Apr 06 23:09:...|NO_QUERY|1467992866|     726|@Wyldceltic1 He h...|      TheBoss63|\n",
      "|65325dcdb4cad9b16...|Mon Apr 06 23:17:...|NO_QUERY|1468018399|     834|Okay, so.. STILL ...|      murielara|\n",
      "|65325dcdb4cad9b16...|Mon Apr 06 23:41:...|NO_QUERY|1468097165|    1154|I suddenly miss m...|        Raaawry|\n",
      "|65325dcdb4cad9b16...|Mon Apr 06 23:44:...|NO_QUERY|1468106339|    1199|Ugh. still workin...|         Syph0n|\n",
      "|65325dcdb4cad9b16...|Tue Apr 07 00:00:...|NO_QUERY|1468154478|    1432|really now, time ...|      supclayyy|\n",
      "|65325dcdb4cad9b16...|Tue Apr 07 00:07:...|NO_QUERY|1468176489|    1545|Too much traffic ...|     JanvdHeide|\n",
      "|65325dcdb4cad9b16...|Tue Apr 07 00:13:...|NO_QUERY|1468192697|    1602|morning all. So t...|      angelroxy|\n",
      "|65325dcdb4cad9b16...|Tue Apr 07 00:20:...|NO_QUERY|1468214434|    1707|@geoffmartinez yo...| lindseyviloria|\n",
      "|65325dcdb4cad9b16...|Tue Apr 07 00:22:...|NO_QUERY|1468220880|    1737|Even a four day w...|Damn_Knit_Blast|\n",
      "|65325dcdb4cad9b16...|Tue Apr 07 00:36:...|NO_QUERY|1468260438|    1907|why do other peop...|      mcsteph94|\n",
      "|65325dcdb4cad9b16...|Tue Apr 07 00:53:...|NO_QUERY|1468308163|    2121|@joenoia wass up ...|    DenzelBurks|\n",
      "|65325dcdb4cad9b16...|Tue Apr 07 01:15:...|NO_QUERY|1468371132|    2399|I am scheduled to...|   MoonFireLove|\n",
      "+--------------------+--------------------+--------+----------+--------+--------------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac2caf36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _id: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- flag: string (nullable = true)\n",
      " |-- ids: long (nullable = true)\n",
      " |-- sequence: integer (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- user: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d189ccab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------+----------+--------+--------------------+---------------+-------------------+\n",
      "|                 _id|                date|    flag|       ids|sequence|                text|           user|  standardized_date|\n",
      "+--------------------+--------------------+--------+----------+--------+--------------------+---------------+-------------------+\n",
      "|65325dcdb4cad9b16...|Mon Apr 06 22:28:...|NO_QUERY|1467843734|     146|my nokia 1110 died..|      chatpataa|2009-04-06 22:28:26|\n",
      "|65325dcdb4cad9b16...|Mon Apr 06 22:36:...|NO_QUERY|1467873980|     246|missed Brent at p...|     gregcronin|2009-04-06 22:36:19|\n",
      "|65325dcdb4cad9b16...|Mon Apr 06 22:40:...|NO_QUERY|1467890212|     301|Mraow, I feel lik...|   MorganWillis|2009-04-06 22:40:43|\n",
      "|65325dcdb4cad9b16...|Mon Apr 06 22:57:...|NO_QUERY|1467951422|     561|sad that the 'fee...|        felloff|2009-04-06 22:57:57|\n",
      "|65325dcdb4cad9b16...|Mon Apr 06 23:00:...|NO_QUERY|1467962336|     592|my heart is broke...|          umfoo|2009-04-06 23:00:55|\n",
      "|65325dcdb4cad9b16...|Mon Apr 06 23:01:...|NO_QUERY|1467962671|     595|I can't take this...|  Bi0hazard2886|2009-04-06 23:01:01|\n",
      "|65325dcdb4cad9b16...|Mon Apr 06 23:06:...|NO_QUERY|1467981027|     663|im sooo sad right...|  maTERIALfemme|2009-04-06 23:06:12|\n",
      "|65325dcdb4cad9b16...|Mon Apr 06 23:08:...|NO_QUERY|1467989188|     714|@stephenkruiser I...|     littlefoxy|2009-04-06 23:08:33|\n",
      "|65325dcdb4cad9b16...|Mon Apr 06 23:09:...|NO_QUERY|1467992866|     726|@Wyldceltic1 He h...|      TheBoss63|2009-04-06 23:09:38|\n",
      "|65325dcdb4cad9b16...|Mon Apr 06 23:17:...|NO_QUERY|1468018399|     834|Okay, so.. STILL ...|      murielara|2009-04-06 23:17:06|\n",
      "|65325dcdb4cad9b16...|Mon Apr 06 23:41:...|NO_QUERY|1468097165|    1154|I suddenly miss m...|        Raaawry|2009-04-06 23:41:52|\n",
      "|65325dcdb4cad9b16...|Mon Apr 06 23:44:...|NO_QUERY|1468106339|    1199|Ugh. still workin...|         Syph0n|2009-04-06 23:44:55|\n",
      "|65325dcdb4cad9b16...|Tue Apr 07 00:00:...|NO_QUERY|1468154478|    1432|really now, time ...|      supclayyy|2009-04-07 00:00:49|\n",
      "|65325dcdb4cad9b16...|Tue Apr 07 00:07:...|NO_QUERY|1468176489|    1545|Too much traffic ...|     JanvdHeide|2009-04-07 00:07:50|\n",
      "|65325dcdb4cad9b16...|Tue Apr 07 00:13:...|NO_QUERY|1468192697|    1602|morning all. So t...|      angelroxy|2009-04-07 00:13:06|\n",
      "|65325dcdb4cad9b16...|Tue Apr 07 00:20:...|NO_QUERY|1468214434|    1707|@geoffmartinez yo...| lindseyviloria|2009-04-07 00:20:22|\n",
      "|65325dcdb4cad9b16...|Tue Apr 07 00:22:...|NO_QUERY|1468220880|    1737|Even a four day w...|Damn_Knit_Blast|2009-04-07 00:22:36|\n",
      "|65325dcdb4cad9b16...|Tue Apr 07 00:36:...|NO_QUERY|1468260438|    1907|why do other peop...|      mcsteph94|2009-04-07 00:36:12|\n",
      "|65325dcdb4cad9b16...|Tue Apr 07 00:53:...|NO_QUERY|1468308163|    2121|@joenoia wass up ...|    DenzelBurks|2009-04-07 00:53:01|\n",
      "|65325dcdb4cad9b16...|Tue Apr 07 01:15:...|NO_QUERY|1468371132|    2399|I am scheduled to...|   MoonFireLove|2009-04-07 01:15:36|\n",
      "+--------------------+--------------------+--------+----------+--------+--------------------+---------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import from_unixtime, unix_timestamp\n",
    "\n",
    "# Set legacy time parser policy\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "\n",
    "# Set the default time zone for the session\n",
    "spark.conf.set(\"spark.sql.session.timeZone\", \"America/Los_Angeles\")\n",
    "\n",
    "input_format = \"E MMM dd HH:mm:ss z yyyy\"\n",
    "standard_format = \"yyyy-MM-dd HH:mm:ss\"\n",
    "\n",
    "# Since we've set the session time zone, the unix_timestamp function will treat the input as in PDT\n",
    "df = df.withColumn(\"standardized_date\", from_unixtime(unix_timestamp(\"date\", input_format), standard_format))\n",
    "\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1028c802",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 2:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|standardized_date  |\n",
      "+-------------------+\n",
      "|2009-04-06 22:28:26|\n",
      "|2009-04-06 22:36:19|\n",
      "|2009-04-06 22:40:43|\n",
      "|2009-04-06 22:57:57|\n",
      "|2009-04-06 23:00:55|\n",
      "|2009-04-06 23:01:01|\n",
      "|2009-04-06 23:06:12|\n",
      "|2009-04-06 23:08:33|\n",
      "|2009-04-06 23:09:38|\n",
      "|2009-04-06 23:17:06|\n",
      "+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.select(\"standardized_date\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cfbb61ce",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:==================================================>        (6 + 1) / 7]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+----------------------+\n",
      "|min(standardized_date)|max(standardized_date)|\n",
      "+----------------------+----------------------+\n",
      "|   2009-04-06 22:28:26|   2009-06-25 10:25:57|\n",
      "+----------------------+----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "df.agg(F.min(\"standardized_date\"), F.max(\"standardized_date\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bd5b4d",
   "metadata": {},
   "source": [
    "- The operation to add a column with a different date format was succesfull based on the last lines of code.\n",
    "\n",
    "- The sequence column also shows that the chronological order was affected during the importation of the dataset, thus the orderBy will be applied to correct the order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "098d67c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:==================================================>        (6 + 1) / 7]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------+----------+--------+--------------------+---------------+-------------------+\n",
      "|                 _id|                date|    flag|       ids|sequence|                text|           user|  standardized_date|\n",
      "+--------------------+--------------------+--------+----------+--------+--------------------+---------------+-------------------+\n",
      "|65325dcdb4cad9b16...|Mon Apr 06 22:28:...|NO_QUERY|1467843734|     146|my nokia 1110 died..|      chatpataa|2009-04-06 22:28:26|\n",
      "|65325dcdb4cad9b16...|Mon Apr 06 22:36:...|NO_QUERY|1467873980|     246|missed Brent at p...|     gregcronin|2009-04-06 22:36:19|\n",
      "|65325dcdb4cad9b16...|Mon Apr 06 22:40:...|NO_QUERY|1467890212|     301|Mraow, I feel lik...|   MorganWillis|2009-04-06 22:40:43|\n",
      "|65325dcdb4cad9b16...|Mon Apr 06 22:57:...|NO_QUERY|1467951422|     561|sad that the 'fee...|        felloff|2009-04-06 22:57:57|\n",
      "|65325dcdb4cad9b16...|Mon Apr 06 23:00:...|NO_QUERY|1467962336|     592|my heart is broke...|          umfoo|2009-04-06 23:00:55|\n",
      "|65325dcdb4cad9b16...|Mon Apr 06 23:01:...|NO_QUERY|1467962671|     595|I can't take this...|  Bi0hazard2886|2009-04-06 23:01:01|\n",
      "|65325dcdb4cad9b16...|Mon Apr 06 23:06:...|NO_QUERY|1467981027|     663|im sooo sad right...|  maTERIALfemme|2009-04-06 23:06:12|\n",
      "|65325dcdb4cad9b16...|Mon Apr 06 23:08:...|NO_QUERY|1467989188|     714|@stephenkruiser I...|     littlefoxy|2009-04-06 23:08:33|\n",
      "|65325dcdb4cad9b16...|Mon Apr 06 23:09:...|NO_QUERY|1467992866|     726|@Wyldceltic1 He h...|      TheBoss63|2009-04-06 23:09:38|\n",
      "|65325dcdb4cad9b16...|Mon Apr 06 23:17:...|NO_QUERY|1468018399|     834|Okay, so.. STILL ...|      murielara|2009-04-06 23:17:06|\n",
      "|65325dcdb4cad9b16...|Mon Apr 06 23:41:...|NO_QUERY|1468097165|    1154|I suddenly miss m...|        Raaawry|2009-04-06 23:41:52|\n",
      "|65325dcdb4cad9b16...|Mon Apr 06 23:44:...|NO_QUERY|1468106339|    1199|Ugh. still workin...|         Syph0n|2009-04-06 23:44:55|\n",
      "|65325dcdb4cad9b16...|Tue Apr 07 00:00:...|NO_QUERY|1468154478|    1432|really now, time ...|      supclayyy|2009-04-07 00:00:49|\n",
      "|65325dcdb4cad9b16...|Tue Apr 07 00:07:...|NO_QUERY|1468176489|    1545|Too much traffic ...|     JanvdHeide|2009-04-07 00:07:50|\n",
      "|65325dcdb4cad9b16...|Tue Apr 07 00:13:...|NO_QUERY|1468192697|    1602|morning all. So t...|      angelroxy|2009-04-07 00:13:06|\n",
      "|65325dcdb4cad9b16...|Tue Apr 07 00:20:...|NO_QUERY|1468214434|    1707|@geoffmartinez yo...| lindseyviloria|2009-04-07 00:20:22|\n",
      "|65325dcdb4cad9b16...|Tue Apr 07 00:22:...|NO_QUERY|1468220880|    1737|Even a four day w...|Damn_Knit_Blast|2009-04-07 00:22:36|\n",
      "|65325dcdb4cad9b16...|Tue Apr 07 00:36:...|NO_QUERY|1468260438|    1907|why do other peop...|      mcsteph94|2009-04-07 00:36:12|\n",
      "|65325dcdb4cad9b16...|Tue Apr 07 00:53:...|NO_QUERY|1468308163|    2121|@joenoia wass up ...|    DenzelBurks|2009-04-07 00:53:01|\n",
      "|65325dcdb4cad9b16...|Tue Apr 07 01:15:...|NO_QUERY|1468371132|    2399|I am scheduled to...|   MoonFireLove|2009-04-07 01:15:36|\n",
      "+--------------------+--------------------+--------+----------+--------+--------------------+---------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df = df.orderBy(col(\"sequence\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60e5a546",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:==================================================>        (6 + 1) / 7]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+\n",
      "|                text|  standardized_date|\n",
      "+--------------------+-------------------+\n",
      "|my nokia 1110 died..|2009-04-06 22:28:26|\n",
      "|missed Brent at p...|2009-04-06 22:36:19|\n",
      "|Mraow, I feel lik...|2009-04-06 22:40:43|\n",
      "|sad that the 'fee...|2009-04-06 22:57:57|\n",
      "|my heart is broke...|2009-04-06 23:00:55|\n",
      "|I can't take this...|2009-04-06 23:01:01|\n",
      "|im sooo sad right...|2009-04-06 23:06:12|\n",
      "|@stephenkruiser I...|2009-04-06 23:08:33|\n",
      "|@Wyldceltic1 He h...|2009-04-06 23:09:38|\n",
      "|Okay, so.. STILL ...|2009-04-06 23:17:06|\n",
      "|I suddenly miss m...|2009-04-06 23:41:52|\n",
      "|Ugh. still workin...|2009-04-06 23:44:55|\n",
      "|really now, time ...|2009-04-07 00:00:49|\n",
      "|Too much traffic ...|2009-04-07 00:07:50|\n",
      "|morning all. So t...|2009-04-07 00:13:06|\n",
      "|@geoffmartinez yo...|2009-04-07 00:20:22|\n",
      "|Even a four day w...|2009-04-07 00:22:36|\n",
      "|why do other peop...|2009-04-07 00:36:12|\n",
      "|@joenoia wass up ...|2009-04-07 00:53:01|\n",
      "|I am scheduled to...|2009-04-07 01:15:36|\n",
      "+--------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = df.drop(\"_id\", \"flag\", \"ids\", \"user\", \"date\", \"sequence\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f8341f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:==================================================>        (6 + 1) / 7]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------------+\n",
      "|text|standardized_date|\n",
      "+----+-----------------+\n",
      "|   0|                0|\n",
      "+----+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import isnull, count, when\n",
    "\n",
    "# Counting missing values for each column\n",
    "missing_counts = df.select([count(when(isnull(c), c)).alias(c) for c in df.columns])\n",
    "\n",
    "missing_counts.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0e374e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Check for duplicate rows based on all columns\n",
    "duplicate_count = df.count() - df.dropDuplicates().count()\n",
    "\n",
    "if duplicate_count > 0:\n",
    "    print(f\"Number of duplicate rows: {duplicate_count}\")\n",
    "    # Remove duplicates and retain only the first occurrence\n",
    "    #df = df.dropDuplicates()\n",
    "    #print(\"Duplicates removed.\")\n",
    "#else:\n",
    "    #print(\"No duplicates found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf88b647",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 20:=================================================>        (6 + 1) / 7]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+\n",
      "|       day|number_of_tweets|\n",
      "+----------+----------------+\n",
      "|2009-04-06|              32|\n",
      "|2009-04-07|             182|\n",
      "|2009-04-17|              26|\n",
      "|2009-04-18|             217|\n",
      "|2009-04-19|             274|\n",
      "|2009-04-20|             183|\n",
      "|2009-04-21|              99|\n",
      "|2009-05-01|              55|\n",
      "|2009-05-02|             275|\n",
      "|2009-05-03|             368|\n",
      "|2009-05-04|             161|\n",
      "|2009-05-09|             121|\n",
      "|2009-05-10|             230|\n",
      "|2009-05-11|              53|\n",
      "|2009-05-13|              37|\n",
      "|2009-05-14|             184|\n",
      "|2009-05-16|              78|\n",
      "|2009-05-17|             378|\n",
      "|2009-05-18|             355|\n",
      "|2009-05-21|              22|\n",
      "+----------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import date_format, count\n",
    "\n",
    "# Grouping by the day and counting\n",
    "tweets_per_day = df.groupBy(date_format(\"standardized_date\", \"yyyy-MM-dd\").alias(\"day\")).agg(count(\"*\").alias(\"number_of_tweets\"))\n",
    "\n",
    "tweets_per_day.orderBy(\"day\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "632a962d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 29:=================================================>        (6 + 1) / 7]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average text length: 73.64880580217581\n",
      "Minimum text length: 6\n",
      "Maximum text length: 170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import length\n",
    "# It checks entries with html tags or url.\n",
    "# Add a column for text length\n",
    "df = df.withColumn(\"text_length\", length(df[\"text\"]))\n",
    "\n",
    "# Compute average, minimum, and maximum text lengths\n",
    "avg_length = df.agg({\"text_length\": \"avg\"}).collect()[0][0]\n",
    "min_length = df.agg({\"text_length\": \"min\"}).collect()[0][0]\n",
    "max_length = df.agg({\"text_length\": \"max\"}).collect()[0][0]\n",
    "\n",
    "print(f\"Average text length: {avg_length}\")\n",
    "print(f\"Minimum text length: {min_length}\")\n",
    "print(f\"Maximum text length: {max_length}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d319dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 38:=================================>                        (4 + 2) / 7]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries with URLs: 726\n",
      "Number of entries with HTML tags: 0\n",
      "Number of entries with mentions: 7518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 38:=========================================>                (5 + 2) / 7]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Count entries with URLs\n",
    "url_count = df.filter(F.col(\"text\").rlike(\"http(s)?://([\\\\w-]+\\\\.)+[\\\\w-]+(/[\\\\w- ./?%&=]*)?\")).count()\n",
    "\n",
    "# Count entries with HTML tags\n",
    "html_tags_count = df.filter(F.col(\"text\").rlike(\"<[^>]+>\")).count()\n",
    "\n",
    "# Count entries with mentions (@username)\n",
    "mentions_count = df.filter(F.col(\"text\").rlike(\"@\\\\w+\")).count()\n",
    "\n",
    "print(f\"Number of entries with URLs: {url_count}\")\n",
    "print(f\"Number of entries with HTML tags: {html_tags_count}\")\n",
    "print(f\"Number of entries with mentions: {mentions_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2213e2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "# Remove URLs\n",
    "df = df.withColumn(\"text\", F.regexp_replace(F.col(\"text\"), \"http(s)?://([\\\\w-]+\\\\.)+[\\\\w-]+(/[\\\\w- ./?%&=]*)?\", \"\"))\n",
    "\n",
    "# Remove HTML tags\n",
    "df = df.withColumn(\"text\", F.regexp_replace(F.col(\"text\"), \"<[^>]+>\", \"\"))\n",
    "\n",
    "# Remove mentions (i.e., @username)\n",
    "df = df.withColumn(\"text\", F.regexp_replace(F.col(\"text\"), \"@\\\\w+\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b577fec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 47:=========================================>                (5 + 2) / 7]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries with URLs: 0\n",
      "Number of entries with HTML tags: 0\n",
      "Number of entries with mentions: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 47:=================================================>        (6 + 1) / 7]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Count entries with URLs\n",
    "url_count = df.filter(F.col(\"text\").rlike(\"http(s)?://([\\\\w-]+\\\\.)+[\\\\w-]+(/[\\\\w- ./?%&=]*)?\")).count()\n",
    "\n",
    "# Count entries with HTML tags\n",
    "html_tags_count = df.filter(F.col(\"text\").rlike(\"<[^>]+>\")).count()\n",
    "\n",
    "# Count entries with mentions (@username)\n",
    "mentions_count = df.filter(F.col(\"text\").rlike(\"@\\\\w+\")).count()\n",
    "\n",
    "print(f\"Number of entries with URLs: {url_count}\")\n",
    "print(f\"Number of entries with HTML tags: {html_tags_count}\")\n",
    "print(f\"Number of entries with mentions: {mentions_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d25e3e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.filter(F.col(\"text\").rlike(\"http(s)?://([\\\\w-]+\\\\.)+[\\\\w-]+(/[\\\\w- ./?%&=]*)?\")).select(\"text\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b481d239",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 50:=========================================>                (5 + 2) / 7]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------------------------------------+\n",
      "|text                                                                                                                            |\n",
      "+--------------------------------------------------------------------------------------------------------------------------------+\n",
      "|my nokia 1110 died..                                                                                                            |\n",
      "|missed Brent at praise band.   No fun to not have the your lead guitarist.  &lt;pout&gt;                                        |\n",
      "|Mraow, I feel like dancing, but first art school wants to rape me some more.                                                    |\n",
      "|sad that the 'feet' of my macbook just fell off : sad that the 'feet' of my macbook just fell off                               |\n",
      "|my heart is broken every morning dropping Foo at pre school, now i understand when moms say &quot;he has my heart broken&quot;. |\n",
      "|I can't take this heat! It's like an oven in here. I feel sick nwo                                                              |\n",
      "|im sooo sad right now  i need a hug                                                                                             |\n",
      "| I'm so sorry to hear that.  It's always sad when we lose those close to us, as we loved them.                                  |\n",
      "| He has Karate tournament in 8 weeks                                                                                            |\n",
      "|Okay, so.. STILL NO SCHOOL!!!!                                                                                                  |\n",
      "|I suddenly miss my Flintstones vitamin tablets  SOOOO GOOOOD                                                                    |\n",
      "|Ugh. still working on project  just taking a small break                                                                        |\n",
      "|really now, time for sleep.  dreaming of my city, more tattoos, and other great things.  waking up to early morning sociology   |\n",
      "|Too much traffic on the A2  Can't wait till all 10 lanes are ready ... 2010?                                                    |\n",
      "|morning all. So tired today, should've stayed in bed                                                                            |\n",
      "| youre going to be in mexico on easter?  why?                                                                                   |\n",
      "|Even a four day week seems too long  I want to stay in bed!                                                                     |\n",
      "|why do other people get replies                                                                                                 |\n",
      "| wass up lovely i anit show you no love yet...                                                                                  |\n",
      "|I am scheduled to be very productive on a few hrs and I still can not sleep.   Insomia has gotten to me..                       |\n",
      "+--------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.select(\"text\").show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab27fb4",
   "metadata": {},
   "source": [
    "# HANDLE EMOJI STAGE, BUT I AM HAVING TROUBLES WITH CODING AND LIBRARY FOR IT. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d9db5220",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ld_wiki_tatoeba_cnn_21 download started this may take some time.\n",
      "Approximate size to download 7.1 MB\n",
      "[ / ]ld_wiki_tatoeba_cnn_21 download started this may take some time.\n",
      "Approximate size to download 7.1 MB\n",
      "[ | ]Download done! Loading the resource.\n",
      "[ \\ ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-27 19:13:48.081528: I external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "from sparknlp.base import DocumentAssembler\n",
    "from sparknlp.annotator import LanguageDetectorDL\n",
    "\n",
    "documentAssembler = DocumentAssembler().setInputCol(\"text\").setOutputCol(\"document\")\n",
    "langDetector = LanguageDetectorDL.pretrained(\"ld_wiki_tatoeba_cnn_21\", \"xx\").setInputCols([\"document\"]).setOutputCol(\"lang\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3d892b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----------+--------------------+--------------------+\n",
      "|                text|  standardized_date|text_length|            document|                lang|\n",
      "+--------------------+-------------------+-----------+--------------------+--------------------+\n",
      "|my nokia 1110 died..|2009-04-06 22:28:26|         20|[{document, 0, 19...|[{language, 0, 19...|\n",
      "|missed Brent at p...|2009-04-06 22:36:19|         88|[{document, 0, 87...|[{language, 0, 87...|\n",
      "|Mraow, I feel lik...|2009-04-06 22:40:43|         77|[{document, 0, 76...|[{language, 0, 76...|\n",
      "|sad that the 'fee...|2009-04-06 22:57:57|         97|[{document, 0, 96...|[{language, 0, 96...|\n",
      "|my heart is broke...|2009-04-06 23:00:55|        128|[{document, 0, 12...|[{language, 0, 12...|\n",
      "|I can't take this...|2009-04-06 23:01:01|         66|[{document, 0, 65...|[{language, 0, 65...|\n",
      "|im sooo sad right...|2009-04-06 23:06:12|         35|[{document, 0, 34...|[{language, 0, 34...|\n",
      "| I'm so sorry to ...|2009-04-06 23:08:33|        109|[{document, 0, 93...|[{language, 0, 93...|\n",
      "| He has Karate to...|2009-04-06 23:09:38|         48|[{document, 0, 35...|[{language, 0, 35...|\n",
      "|Okay, so.. STILL ...|2009-04-06 23:17:06|         31|[{document, 0, 30...|[{language, 0, 30...|\n",
      "|I suddenly miss m...|2009-04-06 23:41:52|         60|[{document, 0, 59...|[{language, 0, 59...|\n",
      "|Ugh. still workin...|2009-04-06 23:44:55|         56|[{document, 0, 55...|[{language, 0, 55...|\n",
      "|really now, time ...|2009-04-07 00:00:49|        126|[{document, 0, 12...|[{language, 0, 12...|\n",
      "|Too much traffic ...|2009-04-07 00:07:50|         76|[{document, 0, 75...|[{language, 0, 75...|\n",
      "|morning all. So t...|2009-04-07 00:13:06|         53|[{document, 0, 52...|[{language, 0, 52...|\n",
      "| youre going to b...|2009-04-07 00:20:22|         59|[{document, 0, 44...|[{language, 0, 44...|\n",
      "|Even a four day w...|2009-04-07 00:22:36|         59|[{document, 0, 58...|[{language, 0, 58...|\n",
      "|why do other peop...|2009-04-07 00:36:12|         31|[{document, 0, 30...|[{language, 0, 30...|\n",
      "| wass up lovely i...|2009-04-07 00:53:01|         54|[{document, 0, 45...|[{language, 0, 45...|\n",
      "|I am scheduled to...|2009-04-07 01:15:36|        105|[{document, 0, 10...|[{language, 0, 10...|\n",
      "+--------------------+-------------------+-----------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline = Pipeline(stages=[documentAssembler, langDetector])\n",
    "model = pipeline.fit(df)\n",
    "df = model.transform(df)\n",
    "\n",
    "df.show()  # This will display the DataFrame with detected languages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dfa432b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df = df.withColumn(\"detected_language\", col(\"lang.result\").getItem(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9f582348",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text',\n",
       " 'standardized_date',\n",
       " 'text_length',\n",
       " 'document',\n",
       " 'lang',\n",
       " 'detected_language']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8d5d2144",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----------+-----------------+\n",
      "|                text|  standardized_date|text_length|detected_language|\n",
      "+--------------------+-------------------+-----------+-----------------+\n",
      "|my nokia 1110 died..|2009-04-06 22:28:26|         20|               en|\n",
      "|missed Brent at p...|2009-04-06 22:36:19|         88|               en|\n",
      "|Mraow, I feel lik...|2009-04-06 22:40:43|         77|               en|\n",
      "|sad that the 'fee...|2009-04-06 22:57:57|         97|               en|\n",
      "|my heart is broke...|2009-04-06 23:00:55|        128|               en|\n",
      "|I can't take this...|2009-04-06 23:01:01|         66|               en|\n",
      "|im sooo sad right...|2009-04-06 23:06:12|         35|               en|\n",
      "| I'm so sorry to ...|2009-04-06 23:08:33|        109|               en|\n",
      "| He has Karate to...|2009-04-06 23:09:38|         48|               en|\n",
      "|Okay, so.. STILL ...|2009-04-06 23:17:06|         31|               en|\n",
      "|I suddenly miss m...|2009-04-06 23:41:52|         60|               en|\n",
      "|Ugh. still workin...|2009-04-06 23:44:55|         56|               en|\n",
      "|really now, time ...|2009-04-07 00:00:49|        126|               en|\n",
      "|Too much traffic ...|2009-04-07 00:07:50|         76|               en|\n",
      "|morning all. So t...|2009-04-07 00:13:06|         53|               en|\n",
      "| youre going to b...|2009-04-07 00:20:22|         59|               en|\n",
      "|Even a four day w...|2009-04-07 00:22:36|         59|               en|\n",
      "|why do other peop...|2009-04-07 00:36:12|         31|               en|\n",
      "| wass up lovely i...|2009-04-07 00:53:01|         54|               en|\n",
      "|I am scheduled to...|2009-04-07 01:15:36|        105|               en|\n",
      "+--------------------+-------------------+-----------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.drop(\"document\", \"lang\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b4fbeecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commented, it takes a long time to run, around half an hour.\n",
    "\n",
    "# Group by detected_language and count the number of tweets for each language\n",
    "#language_counts = df.groupBy(\"detected_language\").agg(F.count(\"text\").alias(\"number_of_tweets\"))\n",
    "\n",
    "# Sort the results by number_of_tweets in descending order for better readability\n",
    "#language_counts = language_counts.sort(F.desc(\"number_of_tweets\"))\n",
    "\n",
    "# Show the results\n",
    "#language_counts.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9ec61719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commented, it takes a long time to run, more than 3 hours.\n",
    "\n",
    "# Get 'en' count from language_counts DataFrame\n",
    "#en_count = language_counts.filter(F.col(\"detected_language\") == \"en\").collect()[0][\"number_of_tweets\"]\n",
    "\n",
    "# Calculate 'non-en' count\n",
    "#non_en_count = language_counts.filter(F.col(\"detected_language\") != \"en\").agg(F.sum(\"number_of_tweets\")).collect()[0][0]\n",
    "\n",
    "#print(f\"Total number of 'en' tweets: {en_count}\")\n",
    "#print(f\"Total number of 'non-en' tweets: {non_en_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "12318151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows where detected_language is not 'en' from the original result DataFrame\n",
    "df = df.filter(F.col(\"detected_language\") == \"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7cba5438",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 56:=================================================>        (6 + 1) / 7]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered count (English tweets only): 15694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Count the total number of observations in the filtered df_eng.\n",
    "en_tweets = df.count()\n",
    "\n",
    "print(f\"Filtered count (English tweets only): {en_tweets}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "637a08d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "\n",
    "# Remove punctuation\n",
    "df = df.withColumn('text', regexp_replace(df['text'], r\"[^\\w\\s]\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "36854131",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lower\n",
    "# Convert to lowercase\n",
    "df = df.withColumn('text', lower(df['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "321fdb75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 59:=================================================>        (6 + 1) / 7]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------------------------------+\n",
      "|text                                                                                                                      |\n",
      "+--------------------------------------------------------------------------------------------------------------------------+\n",
      "|my nokia 1110 died                                                                                                        |\n",
      "|missed brent at praise band   no fun to not have the your lead guitarist  ltpoutgt                                        |\n",
      "|mraow i feel like dancing but first art school wants to rape me some more                                                 |\n",
      "|sad that the feet of my macbook just fell off  sad that the feet of my macbook just fell off                              |\n",
      "|my heart is broken every morning dropping foo at pre school now i understand when moms say quothe has my heart brokenquot |\n",
      "|i cant take this heat its like an oven in here i feel sick nwo                                                            |\n",
      "|im sooo sad right now  i need a hug                                                                                       |\n",
      "| im so sorry to hear that  its always sad when we lose those close to us as we loved them                                 |\n",
      "| he has karate tournament in 8 weeks                                                                                      |\n",
      "|okay so still no school                                                                                                   |\n",
      "|i suddenly miss my flintstones vitamin tablets  soooo gooood                                                              |\n",
      "|ugh still working on project  just taking a small break                                                                   |\n",
      "|really now time for sleep  dreaming of my city more tattoos and other great things  waking up to early morning sociology  |\n",
      "|too much traffic on the a2  cant wait till all 10 lanes are ready  2010                                                   |\n",
      "|morning all so tired today shouldve stayed in bed                                                                         |\n",
      "| youre going to be in mexico on easter  why                                                                               |\n",
      "|even a four day week seems too long  i want to stay in bed                                                                |\n",
      "|why do other people get replies                                                                                           |\n",
      "| wass up lovely i anit show you no love yet                                                                               |\n",
      "|i am scheduled to be very productive on a few hrs and i still can not sleep   insomia has gotten to me                    |\n",
      "+--------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.select(\"text\").show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5c6e3014",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 60:=================================================>        (6 + 1) / 7]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|tokens                                                                                                                                          |\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[my, nokia, 1110, died]                                                                                                                         |\n",
      "|[missed, brent, at, praise, band, no, fun, to, not, have, the, your, lead, guitarist, ltpoutgt]                                                 |\n",
      "|[mraow, i, feel, like, dancing, but, first, art, school, wants, to, rape, me, some, more]                                                       |\n",
      "|[sad, that, the, feet, of, my, macbook, just, fell, off, sad, that, the, feet, of, my, macbook, just, fell, off]                                |\n",
      "|[my, heart, is, broken, every, morning, dropping, foo, at, pre, school, now, i, understand, when, moms, say, quothe, has, my, heart, brokenquot]|\n",
      "|[i, cant, take, this, heat, its, like, an, oven, in, here, i, feel, sick, nwo]                                                                  |\n",
      "|[im, sooo, sad, right, now, i, need, a, hug]                                                                                                    |\n",
      "|[im, so, sorry, to, hear, that, its, always, sad, when, we, lose, those, close, to, us, as, we, loved, them]                                    |\n",
      "|[he, has, karate, tournament, in, 8, weeks]                                                                                                     |\n",
      "|[okay, so, still, no, school]                                                                                                                   |\n",
      "|[i, suddenly, miss, my, flintstones, vitamin, tablets, soooo, gooood]                                                                           |\n",
      "|[ugh, still, working, on, project, just, taking, a, small, break]                                                                               |\n",
      "|[really, now, time, for, sleep, dreaming, of, my, city, more, tattoos, and, other, great, things, waking, up, to, early, morning, sociology]    |\n",
      "|[too, much, traffic, on, the, a2, cant, wait, till, all, 10, lanes, are, ready, 2010]                                                           |\n",
      "|[morning, all, so, tired, today, shouldve, stayed, in, bed]                                                                                     |\n",
      "|[youre, going, to, be, in, mexico, on, easter, why]                                                                                             |\n",
      "|[even, a, four, day, week, seems, too, long, i, want, to, stay, in, bed]                                                                        |\n",
      "|[why, do, other, people, get, replies]                                                                                                          |\n",
      "|[wass, up, lovely, i, anit, show, you, no, love, yet]                                                                                           |\n",
      "|[i, am, scheduled, to, be, very, productive, on, a, few, hrs, and, i, still, can, not, sleep, insomia, has, gotten, to, me]                     |\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer\n",
    "\n",
    "# Initialize a regex tokenizer\n",
    "regex_tokenizer = RegexTokenizer(inputCol=\"text\", outputCol=\"tokens\", pattern=\"\\\\W+\")\n",
    "\n",
    "# Transform the dataset\n",
    "tokens = regex_tokenizer.transform(df)\n",
    "tokens.select(\"tokens\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2ba8ae42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 61:=================================================>        (6 + 1) / 7]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------+\n",
      "|filtered_tokens                                                                                              |\n",
      "+-------------------------------------------------------------------------------------------------------------+\n",
      "|[nokia, 1110, died]                                                                                          |\n",
      "|[missed, brent, praise, band, fun, lead, guitarist, ltpoutgt]                                                |\n",
      "|[mraow, feel, like, dancing, first, art, school, wants, rape]                                                |\n",
      "|[sad, feet, macbook, fell, sad, feet, macbook, fell]                                                         |\n",
      "|[heart, broken, every, morning, dropping, foo, pre, school, understand, moms, say, quothe, heart, brokenquot]|\n",
      "|[cant, take, heat, like, oven, feel, sick, nwo]                                                              |\n",
      "|[im, sooo, sad, right, need, hug]                                                                            |\n",
      "|[im, sorry, hear, always, sad, lose, close, us, loved]                                                       |\n",
      "|[karate, tournament, 8, weeks]                                                                               |\n",
      "|[okay, still, school]                                                                                        |\n",
      "|[suddenly, miss, flintstones, vitamin, tablets, soooo, gooood]                                               |\n",
      "|[ugh, still, working, project, taking, small, break]                                                         |\n",
      "|[really, time, sleep, dreaming, city, tattoos, great, things, waking, early, morning, sociology]             |\n",
      "|[much, traffic, a2, cant, wait, till, 10, lanes, ready, 2010]                                                |\n",
      "|[morning, tired, today, shouldve, stayed, bed]                                                               |\n",
      "|[youre, going, mexico, easter]                                                                               |\n",
      "|[even, four, day, week, seems, long, want, stay, bed]                                                        |\n",
      "|[people, get, replies]                                                                                       |\n",
      "|[wass, lovely, anit, show, love, yet]                                                                        |\n",
      "|[scheduled, productive, hrs, still, sleep, insomia, gotten]                                                  |\n",
      "+-------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "# Initialize a stopwords remover\n",
    "remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered_tokens\")\n",
    "\n",
    "# Transform the tokenized data\n",
    "df = remover.transform(tokens)\n",
    "df.select(\"filtered_tokens\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "32f7cffa",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----------+-----------------+--------------------+--------------------+\n",
      "|                text|  standardized_date|text_length|detected_language|              tokens|     filtered_tokens|\n",
      "+--------------------+-------------------+-----------+-----------------+--------------------+--------------------+\n",
      "|  my nokia 1110 died|2009-04-06 22:28:26|         20|               en|[my, nokia, 1110,...| [nokia, 1110, died]|\n",
      "|missed brent at p...|2009-04-06 22:36:19|         88|               en|[missed, brent, a...|[missed, brent, p...|\n",
      "|mraow i feel like...|2009-04-06 22:40:43|         77|               en|[mraow, i, feel, ...|[mraow, feel, lik...|\n",
      "|sad that the feet...|2009-04-06 22:57:57|         97|               en|[sad, that, the, ...|[sad, feet, macbo...|\n",
      "|my heart is broke...|2009-04-06 23:00:55|        128|               en|[my, heart, is, b...|[heart, broken, e...|\n",
      "|i cant take this ...|2009-04-06 23:01:01|         66|               en|[i, cant, take, t...|[cant, take, heat...|\n",
      "|im sooo sad right...|2009-04-06 23:06:12|         35|               en|[im, sooo, sad, r...|[im, sooo, sad, r...|\n",
      "| im so sorry to h...|2009-04-06 23:08:33|        109|               en|[im, so, sorry, t...|[im, sorry, hear,...|\n",
      "| he has karate to...|2009-04-06 23:09:38|         48|               en|[he, has, karate,...|[karate, tourname...|\n",
      "|okay so still no ...|2009-04-06 23:17:06|         31|               en|[okay, so, still,...|[okay, still, sch...|\n",
      "|i suddenly miss m...|2009-04-06 23:41:52|         60|               en|[i, suddenly, mis...|[suddenly, miss, ...|\n",
      "|ugh still working...|2009-04-06 23:44:55|         56|               en|[ugh, still, work...|[ugh, still, work...|\n",
      "|really now time f...|2009-04-07 00:00:49|        126|               en|[really, now, tim...|[really, time, sl...|\n",
      "|too much traffic ...|2009-04-07 00:07:50|         76|               en|[too, much, traff...|[much, traffic, a...|\n",
      "|morning all so ti...|2009-04-07 00:13:06|         53|               en|[morning, all, so...|[morning, tired, ...|\n",
      "| youre going to b...|2009-04-07 00:20:22|         59|               en|[youre, going, to...|[youre, going, me...|\n",
      "|even a four day w...|2009-04-07 00:22:36|         59|               en|[even, a, four, d...|[even, four, day,...|\n",
      "|why do other peop...|2009-04-07 00:36:12|         31|               en|[why, do, other, ...|[people, get, rep...|\n",
      "| wass up lovely i...|2009-04-07 00:53:01|         54|               en|[wass, up, lovely...|[wass, lovely, an...|\n",
      "|i am scheduled to...|2009-04-07 01:15:36|        105|               en|[i, am, scheduled...|[scheduled, produ...|\n",
      "+--------------------+-------------------+-----------+-----------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d9d5ea94",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text',\n",
       " 'standardized_date',\n",
       " 'text_length',\n",
       " 'detected_language',\n",
       " 'tokens',\n",
       " 'filtered_tokens',\n",
       " 'lemmatized_tokens']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17cd0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization does not run, serialization udf issue? \n",
    "# TypeError: code() argument 13 must be str, not into \n",
    "aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b99c4906",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/hduser/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "2023-10-27 20:27:50,800 ERROR executor.Executor: Exception in task 0.0 in stage 66.0 (TID 226)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 603, in main\n",
      "    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n",
      "                                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 449, in read_udfs\n",
      "    udfs.append(read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=i))\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 251, in read_single_udf\n",
      "    f, return_type = read_command(pickleSer, infile)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 71, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 160, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 430, in loads\n",
      "    return pickle.loads(obj, encoding=encoding)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: code() argument 13 must be str, not int\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:556)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:86)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:68)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:509)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:350)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "2023-10-27 20:27:50,845 WARN scheduler.TaskSetManager: Lost task 0.0 in stage 66.0 (TID 226) (10.0.2.15 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 603, in main\n",
      "    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n",
      "                                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 449, in read_udfs\n",
      "    udfs.append(read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=i))\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 251, in read_single_udf\n",
      "    f, return_type = read_command(pickleSer, infile)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 71, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 160, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 430, in loads\n",
      "    return pickle.loads(obj, encoding=encoding)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: code() argument 13 must be str, not int\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:556)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:86)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:68)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:509)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:350)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "2023-10-27 20:27:50,855 ERROR scheduler.TaskSetManager: Task 0 in stage 66.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 603, in main\n    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n                                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 449, in read_udfs\n    udfs.append(read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=i))\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 251, in read_single_udf\n    f, return_type = read_command(pickleSer, infile)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 71, in read_command\n    command = serializer._read_with_length(file)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 160, in _read_with_length\n    return self.loads(obj)\n           ^^^^^^^^^^^^^^^\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 430, in loads\n    return pickle.loads(obj, encoding=encoding)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nTypeError: code() argument 13 must be str, not int\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlemmatized_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m, lemmatize_udf(df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfiltered_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Show the resulting DataFrame\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m df\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlemmatized_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py:494\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    491\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParameter \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must be a bool\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 494\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mshowString(n, \u001b[38;5;241m20\u001b[39m, vertical))\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    496\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    113\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 603, in main\n    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n                                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 449, in read_udfs\n    udfs.append(read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=i))\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 251, in read_single_udf\n    f, return_type = read_command(pickleSer, infile)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 71, in read_command\n    command = serializer._read_with_length(file)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 160, in _read_with_length\n    return self.loads(obj)\n           ^^^^^^^^^^^^^^^\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 430, in loads\n    return pickle.loads(obj, encoding=encoding)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nTypeError: code() argument 13 must be str, not int\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Initialize the NLTK Lemmatizer\n",
    "nltk.download(\"wordnet\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Define a function to perform lemmatization using NLTK\n",
    "def lemmatize_tokens(tokens):\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "# Register the UDF\n",
    "lemmatize_udf = udf(lemmatize_tokens, ArrayType(StringType()))\n",
    "\n",
    "# Apply the UDF to the DataFrame\n",
    "df = df.withColumn(\"lemmatized_tokens\", lemmatize_udf(df[\"filtered_tokens\"]))\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "df.select(\"lemmatized_tokens\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3301f314",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "import spacy\n",
    "\n",
    "# Define a function to perform lemmatization using spaCy\n",
    "def lemmatize_tokens(tokens):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(\" \".join(tokens))\n",
    "    return [token.lemma_ for token in doc]\n",
    "\n",
    "# Register the UDF\n",
    "lemmatize_udf = udf(lemmatize_tokens, ArrayType(StringType()))\n",
    "\n",
    "# Apply the UDF to the DataFrame\n",
    "df = df.withColumn(\"lemmatized_tokens\", lemmatize_udf(df[\"filtered_tokens\"]))\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "df.select(\"lemmatized_tokens\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d82337c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cf10ac5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(inputCol=\"filtered_tokens\", outputCol=\"term_frequency\")\n",
    "model = cv.fit(df)\n",
    "df = model.transform(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2ad819fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 79:=================================================>        (6 + 1) / 7]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|     filtered_tokens|      term_frequency|\n",
      "+--------------------+--------------------+\n",
      "| [nokia, 1110, died]|(19911,[623,3422,...|\n",
      "|[missed, brent, p...|(19911,[45,107,79...|\n",
      "|[mraow, feel, lik...|(19911,[4,42,87,8...|\n",
      "|[sad, feet, macbo...|(19911,[46,515,81...|\n",
      "|[heart, broken, e...|(19911,[38,87,88,...|\n",
      "|[cant, take, heat...|(19911,[4,8,42,77...|\n",
      "|[im, sooo, sad, r...|(19911,[0,34,46,4...|\n",
      "|[im, sorry, hear,...|(19911,[0,46,53,8...|\n",
      "|[karate, tourname...|(19911,[278,466,4...|\n",
      "|[okay, still, sch...|(19911,[22,87,248...|\n",
      "|[suddenly, miss, ...|(19911,[35,511,26...|\n",
      "|[ugh, still, work...|(19911,[22,80,234...|\n",
      "|[really, time, sl...|(19911,[12,17,36,...|\n",
      "|[much, traffic, a...|(19911,[8,30,69,1...|\n",
      "|[morning, tired, ...|(19911,[6,38,67,1...|\n",
      "|[youre, going, me...|(19911,[10,68,191...|\n",
      "|[even, four, day,...|(19911,[2,23,67,7...|\n",
      "|[people, get, rep...|(19911,[1,73,2852...|\n",
      "|[wass, lovely, an...|(19911,[13,113,12...|\n",
      "|[scheduled, produ...|(19911,[22,54,947...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.select(\"filtered_tokens\", \"term_frequency\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0591221f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import IDF\n",
    "\n",
    "idf = IDF(inputCol=\"term_frequency\", outputCol=\"tfidf\")\n",
    "idf_model = idf.fit(df)\n",
    "df = idf_model.transform(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ebe45fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|     filtered_tokens|      term_frequency|               tfidf|\n",
      "+--------------------+--------------------+--------------------+\n",
      "| [nokia, 1110, died]|(19911,[623,3422,...|(19911,[623,3422,...|\n",
      "|[missed, brent, p...|(19911,[45,107,79...|(19911,[45,107,79...|\n",
      "|[mraow, feel, lik...|(19911,[4,42,87,8...|(19911,[4,42,87,8...|\n",
      "|[sad, feet, macbo...|(19911,[46,515,81...|(19911,[46,515,81...|\n",
      "|[heart, broken, e...|(19911,[38,87,88,...|(19911,[38,87,88,...|\n",
      "|[cant, take, heat...|(19911,[4,8,42,77...|(19911,[4,8,42,77...|\n",
      "|[im, sooo, sad, r...|(19911,[0,34,46,4...|(19911,[0,34,46,4...|\n",
      "|[im, sorry, hear,...|(19911,[0,46,53,8...|(19911,[0,46,53,8...|\n",
      "|[karate, tourname...|(19911,[278,466,4...|(19911,[278,466,4...|\n",
      "|[okay, still, sch...|(19911,[22,87,248...|(19911,[22,87,248...|\n",
      "|[suddenly, miss, ...|(19911,[35,511,26...|(19911,[35,511,26...|\n",
      "|[ugh, still, work...|(19911,[22,80,234...|(19911,[22,80,234...|\n",
      "|[really, time, sl...|(19911,[12,17,36,...|(19911,[12,17,36,...|\n",
      "|[much, traffic, a...|(19911,[8,30,69,1...|(19911,[8,30,69,1...|\n",
      "|[morning, tired, ...|(19911,[6,38,67,1...|(19911,[6,38,67,1...|\n",
      "|[youre, going, me...|(19911,[10,68,191...|(19911,[10,68,191...|\n",
      "|[even, four, day,...|(19911,[2,23,67,7...|(19911,[2,23,67,7...|\n",
      "|[people, get, rep...|(19911,[1,73,2852...|(19911,[1,73,2852...|\n",
      "|[wass, lovely, an...|(19911,[13,113,12...|(19911,[13,113,12...|\n",
      "|[scheduled, produ...|(19911,[22,54,947...|(19911,[22,54,947...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"filtered_tokens\", \"term_frequency\", \"tfidf\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64607809",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
