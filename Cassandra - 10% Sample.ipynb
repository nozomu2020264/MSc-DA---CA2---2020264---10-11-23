{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7ce788d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CassandraIntegration\") \\\n",
    "    .config(\"spark.cassandra.connection.host\", \"127.0.0.1\") \\\n",
    "    .config(\"spark.cassandra.connection.port\", \"9042\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a64ae8e4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to import dataset: 12.620269775390625 seconds.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "df = spark.read \\\n",
    "    .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "    .options(table=\"tweets\", keyspace=\"tweets\") \\\n",
    "    .load()\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Time taken to import dataset: {end_time - start_time} seconds.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c378c8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_df = df.sample(withReplacement=False, fraction=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3226288a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+--------+----------+--------------------+---------------+\n",
      "|sequence|              dates|    flag|       ids|                text|           user|\n",
      "+--------+-------------------+--------+----------+--------------------+---------------+\n",
      "| 1318682|2009-06-03 02:24:16|NO_QUERY|2014399896|\"baby c ulater. =...|     Mc_Rhi_Rhi|\n",
      "| 1259344|2009-06-01 18:58:26|NO_QUERY|1998088014|\"@savannahflower ...|  amazingbianca|\n",
      "| 1260717|2009-06-01 19:32:19|NO_QUERY|1998432163|\"@LisaGemini nigh...|    SashaHalima|\n",
      "|  391292|2009-06-06 09:15:26|NO_QUERY|2054875139|\"revising for my ...|      melisaaxx|\n",
      "| 1151159|2009-05-31 00:39:42|NO_QUERY|1978534140|\"Having an amazin...|  hillarymiller|\n",
      "|  378678|2009-06-06 00:51:02|NO_QUERY|2052162114|\"@Dave_Chappelle ...|Luciddreamrc123|\n",
      "| 1383545|2009-06-06 02:13:09|NO_QUERY|2052578229|\"Right off to LGB...|     NUS_Elaine|\n",
      "| 1494376|2009-06-07 17:19:20|NO_QUERY|2069677585|\"@TheWilson77 wel...|   pixeltickler|\n",
      "|  643258|2009-06-19 02:55:07|NO_QUERY|2235723280|\"Cash Machine upd...|       DerekFME|\n",
      "| 1261348|2009-06-01 19:48:37|NO_QUERY|1998594832|\"@kaitlynm it's o...|       ryankage|\n",
      "| 1477465|2009-06-07 11:15:46|NO_QUERY|2066302567|\"@saragarth hello...|    joshtastic1|\n",
      "|  828108|2009-04-19 02:17:34|NO_QUERY|1556970460|\"Fuck yes. Distor...|       MegLizzy|\n",
      "| 1069647|2009-05-29 17:52:34|NO_QUERY|1965966956|\"@Jeslikeme haha ...|    DavidStella|\n",
      "| 1154824|2009-05-31 02:03:54|NO_QUERY|1978954599|\"@crrystalbabe Gu...|        xNitsuj|\n",
      "|  937637|2009-05-14 03:01:42|NO_QUERY|1793267738|\"@benrolfe QUICK!...|   lloydengland|\n",
      "| 1171004|2009-05-31 07:45:41|NO_QUERY|1980443790|\"@wishbearxx_ You...|     jadmonsod9|\n",
      "| 1234982|2009-06-01 09:51:02|NO_QUERY|1992497864|\"@MHMason just do...|         jyeags|\n",
      "|  498541|2009-06-15 19:33:26|NO_QUERY|2186240959|\"@msilve I didn't...|   fifthand56th|\n",
      "|  846379|2009-04-20 04:13:41|NO_QUERY|1564383944|\"Is glad to be ba...|         nutz79|\n",
      "| 1018730|2009-05-22 07:09:18|NO_QUERY|1882062376|\"@AlisaHofer Than...| CandaceCalvert|\n",
      "+--------+-------------------+--------+----------+--------------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sampled_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70445571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sequence: integer (nullable = false)\n",
      " |-- dates: timestamp (nullable = true)\n",
      " |-- flag: string (nullable = true)\n",
      " |-- ids: long (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- user: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sampled_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7da9fae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:==================================================>        (6 + 1) / 7]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+--------+----------+--------------------+---------------+\n",
      "|sequence|              dates|    flag|       ids|                text|           user|\n",
      "+--------+-------------------+--------+----------+--------------------+---------------+\n",
      "|      21|2009-04-06 23:20:38|NO_QUERY|1467813992|\"one of my friend...|     swinspeedx|\n",
      "|      32|2009-04-06 23:21:09|NO_QUERY|1467815988|\"thought sleeping...|       merisssa|\n",
      "|      66|2009-04-06 23:23:23|NO_QUERY|1467824664|\"Damm back to sch...|     a_mariepyt|\n",
      "|      90|2009-04-06 23:26:10|NO_QUERY|1467835345|\"@Hollywoodheat I...|     RU_it_girl|\n",
      "|     117|2009-04-06 23:27:16|NO_QUERY|1467839450|\"ugh. cant sleep....|  BreannaBonana|\n",
      "|     160|2009-04-06 23:30:54|NO_QUERY|1467853356|\"Picked Mich St t...|       dbmendel|\n",
      "|     574|2009-04-06 23:58:33|NO_QUERY|1467953500|\"@ballinbitch hah...|     candilaria|\n",
      "|     661|2009-04-07 00:06:08|NO_QUERY|1467980858|\"sad day: manu ou...|    Casey_Szulc|\n",
      "|     732|2009-04-07 00:10:03|NO_QUERY|1467994306|\"im so tired of w...|   katiehufford|\n",
      "|    1100|2009-04-07 00:36:10|NO_QUERY|1468079864|\"Morning all, sta...|        sarahgb|\n",
      "|    1129|2009-04-07 00:38:42|NO_QUERY|1468087596|\"Sigh, I think my...|    ScribblesNZ|\n",
      "|    1231|2009-04-07 00:46:56|NO_QUERY|1468112539|   \"@thecoolestout \"|    MissPassion|\n",
      "|    1279|2009-04-07 00:49:49|NO_QUERY|1468121052|\"@TheBlondeTheory...|       angelarg|\n",
      "|    1322|2009-04-07 00:52:49|NO_QUERY|1468130215|\"uh...I feel so l...|     sayekuna10|\n",
      "|    1326|2009-04-07 00:52:56|NO_QUERY|1468130353|\"has to return th...|      halli0400|\n",
      "|    1332|2009-04-07 00:53:20|NO_QUERY|1468131634|\"feels a headache...|melissaaaaaaaaa|\n",
      "|    1335|2009-04-07 00:53:26|NO_QUERY|1468131926|\"@marnieblaze hah...|    teganpearce|\n",
      "|    1878|2009-04-07 01:33:53|NO_QUERY|1468253642|\"on the coach  go...| somanydialects|\n",
      "|    1891|2009-04-07 01:35:20|NO_QUERY|1468258078|\"pose ta b goin t...|         msbady|\n",
      "|    1917|2009-04-07 01:36:35|NO_QUERY|1468261518|\"baking oatmeal +...|      CloverMay|\n",
      "+--------+-------------------+--------+----------+--------------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "sampled_df = sampled_df.orderBy(col(\"sequence\"))\n",
    "sampled_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ad71693",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:==================================================>        (6 + 1) / 7]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+\n",
      "|              dates|                text|\n",
      "+-------------------+--------------------+\n",
      "|2009-04-06 23:28:23|\"Was intending to...|\n",
      "|2009-04-06 23:32:16|\"Just woke up an ...|\n",
      "|2009-04-06 23:40:26|\"@ashleyskyy but ...|\n",
      "|2009-04-06 23:42:49|\"I swear no matte...|\n",
      "|2009-04-06 23:45:08|\"Any chance  Soft...|\n",
      "|2009-04-07 00:11:55|\"Just picked up s...|\n",
      "|2009-04-07 00:12:31|\"i miss being at ...|\n",
      "|2009-04-07 00:25:54|\"Gah, Comcast dou...|\n",
      "|2009-04-07 00:38:45|\"@doriantaylor I ...|\n",
      "|2009-04-07 00:42:15|\"i forgot how to ...|\n",
      "|2009-04-07 00:43:02|\"Not to self: lic...|\n",
      "|2009-04-07 00:45:09|\"Today was a less...|\n",
      "|2009-04-07 00:45:42|\"@Kimboinlimbo no...|\n",
      "|2009-04-07 00:48:41|\"@missoliviaa noo...|\n",
      "|2009-04-07 00:51:13|\"@LarrissaR pleas...|\n",
      "|2009-04-07 00:59:45|\"@LRon_Jaii LMAOO...|\n",
      "|2009-04-07 01:19:35|\"@trash_kitten  s...|\n",
      "|2009-04-07 01:23:36|\"@gabysslave than...|\n",
      "|2009-04-07 01:27:15|\"@valonthecoast L...|\n",
      "|2009-04-07 01:52:36|\"Blazing row with...|\n",
      "+-------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sampled_df = sampled_df.drop(\"_id\", \"flag\", \"ids\", \"user\", \"date\", \"sequence\")\n",
    "sampled_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aea1c614",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:==================================================>        (6 + 1) / 7]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+\n",
      "|dates|text|\n",
      "+-----+----+\n",
      "|    0|   0|\n",
      "+-----+----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import isnull, count, when\n",
    "\n",
    "# Counting missing values for each column\n",
    "missing_counts = sampled_df.select([count(when(isnull(c), c)).alias(c) for c in sampled_df.columns])\n",
    "\n",
    "missing_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a459713",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 12:=================================================>        (6 + 1) / 7]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries with URLs: 70068\n",
      "Number of entries with HTML tags: 0\n",
      "Number of entries with mentions: 738491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Count entries with URLs\n",
    "url_count = df.filter(F.col(\"text\").rlike(\"http(s)?://([\\\\w-]+\\\\.)+[\\\\w-]+(/[\\\\w- ./?%&=]*)?\")).count()\n",
    "\n",
    "# Count entries with HTML tags\n",
    "html_tags_count = df.filter(F.col(\"text\").rlike(\"<[^>]+>\")).count()\n",
    "\n",
    "# Count entries with mentions (@username)\n",
    "mentions_count = df.filter(F.col(\"text\").rlike(\"@\\\\w+\")).count()\n",
    "\n",
    "print(f\"Number of entries with URLs: {url_count}\")\n",
    "print(f\"Number of entries with HTML tags: {html_tags_count}\")\n",
    "print(f\"Number of entries with mentions: {mentions_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ae28d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "# Remove URLs\n",
    "sampled_df = sampled_df.withColumn(\"text\", F.regexp_replace(F.col(\"text\"), \"http(s)?://([\\\\w-]+\\\\.)+[\\\\w-]+(/[\\\\w- ./?%&=]*)?\", \"\"))\n",
    "\n",
    "# Remove HTML tags\n",
    "sampled_df = sampled_df.withColumn(\"text\", F.regexp_replace(F.col(\"text\"), \"<[^>]+>\", \"\"))\n",
    "\n",
    "# Remove mentions (i.e., @username)\n",
    "sampled_df = sampled_df.withColumn(\"text\", F.regexp_replace(F.col(\"text\"), \"@\\\\w+\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01e83c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 21:=================================================>        (6 + 1) / 7]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries with URLs: 0\n",
      "Number of entries with HTML tags: 0\n",
      "Number of entries with mentions: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Count entries with URLs\n",
    "url_count = sampled_df.filter(F.col(\"text\").rlike(\"http(s)?://([\\\\w-]+\\\\.)+[\\\\w-]+(/[\\\\w- ./?%&=]*)?\")).count()\n",
    "\n",
    "# Count entries with HTML tags\n",
    "html_tags_count = sampled_df.filter(F.col(\"text\").rlike(\"<[^>]+>\")).count()\n",
    "\n",
    "# Count entries with mentions (@username)\n",
    "mentions_count = sampled_df.filter(F.col(\"text\").rlike(\"@\\\\w+\")).count()\n",
    "\n",
    "print(f\"Number of entries with URLs: {url_count}\")\n",
    "print(f\"Number of entries with HTML tags: {html_tags_count}\")\n",
    "print(f\"Number of entries with mentions: {mentions_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3fc381cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If there is any special character left.\n",
    "# df.filter(F.col(\"text\").rlike(\"http(s)?://([\\\\w-]+\\\\.)+[\\\\w-]+(/[\\\\w- ./?%&=]*)?\")).select(\"text\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e9e5025",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 27:=================================================>        (6 + 1) / 7]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows: 58\n",
      "Duplicates removed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Check for duplicate rows based on all columns\n",
    "duplicate_count = sampled_df.count() - sampled_df.dropDuplicates().count()\n",
    "\n",
    "if duplicate_count > 0:\n",
    "    print(f\"Number of duplicate rows: {duplicate_count}\")\n",
    "    # Remove duplicates and retain only the first occurrence\n",
    "    sampled_df = sampled_df.dropDuplicates()\n",
    "    print(\"Duplicates removed.\")\n",
    "else:\n",
    "    print(\"No duplicates found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b7bb471",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 33:=================================================>        (6 + 1) / 7]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------------------------------------+\n",
      "|text                                                                                                                          |\n",
      "+------------------------------------------------------------------------------------------------------------------------------+\n",
      "|\" just stop it with the menu.  I did NOT get an invite. *nose in air* I don't spoil barbecues!!\"                              |\n",
      "|\"Why are CDs in HMV so overpriced? ï¿½12.99 for ? I hate having no money \"                                                    |\n",
      "|\"around, hills finale tonight \"                                                                                               |\n",
      "|\" Thanks Alex! Hey everyone, check out the official 'New Moon' trailer  \"                                                     |\n",
      "|\"Somehow glad Mine That Bird didn't win Belmont - I'd have to be pissed at Rachel Alexandra as a spoiler....  lol \"           |\n",
      "|\"i am not ready for all my friends to leave me yet \"                                                                          |\n",
      "|\"My new summer jacket by    \"                                                                                                 |\n",
      "|\"just had lunch.... \"                                                                                                         |\n",
      "|\"My spelling is terrible (RECOMMENDED) not reccommended \"                                                                     |\n",
      "|\"Stayed up till 6am yesterday...don't even know how I'm awake right now. But yay, time for more studying. \"                   |\n",
      "|\"  were trying to but me and c-money's schedules are all out of whack \"                                                       |\n",
      "|\"our film came 2nd in 24 hours film challenge...Very happy and proud of our achievement ..well done everybody \"               |\n",
      "|\"New interesting people out here. Should go read some blogs but have to work today. Hello new people though!!!! \"             |\n",
      "|\"Watching: &quot;StephenieMeyer.com | Twilight Series | New Moon | The Movie trailer&quot; Ainda falta tanto tempo  ( )\"      |\n",
      "|\"Hey  we've either got some real good ledge-to-nest flight or one has been airborne already   (hawkcam live &gt; )\"           |\n",
      "|\" i havent seen it but i can feel the pain \"                                                                                  |\n",
      "|\"Woah!!!! Have I even seen a more amazing sunset with my own eyes? Where is my camera?!! \"                                    |\n",
      "|\" It really makes me sad everytime other fans get replies from you while I don't. I really love McFly.  Just one shoutout!! (\"|\n",
      "|\" how about a new article on finding your YouTube channel stats in your Google Analytics account? Can't find it \"             |\n",
      "|\"Rip my another nig of mine! Damn, all the good ones. Wtf. \"                                                                  |\n",
      "+------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sampled_df.select(\"text\").show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bda076a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ld_wiki_tatoeba_cnn_21 download started this may take some time.\n",
      "Approximate size to download 7.1 MB\n",
      "[ / ]ld_wiki_tatoeba_cnn_21 download started this may take some time.\n",
      "Approximate size to download 7.1 MB\n",
      "[ — ]Download done! Loading the resource.\n",
      "[ / ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-29 22:07:07.304935: I external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "from sparknlp.base import DocumentAssembler\n",
    "from sparknlp.annotator import LanguageDetectorDL\n",
    "\n",
    "documentAssembler = DocumentAssembler().setInputCol(\"text\").setOutputCol(\"document\")\n",
    "langDetector = LanguageDetectorDL.pretrained(\"ld_wiki_tatoeba_cnn_21\", \"xx\").setInputCols([\"document\"]).setOutputCol(\"lang\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0dc9aa56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 45:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+--------------------+--------------------+\n",
      "|              dates|                text|            document|                lang|\n",
      "+-------------------+--------------------+--------------------+--------------------+\n",
      "|2009-04-07 01:30:32|\"i want a guy lik...|[{document, 0, 37...|[{language, 0, 37...|\n",
      "|2009-04-19 08:29:42|\"Twitter API dead...|[{document, 0, 64...|[{language, 0, 64...|\n",
      "|2009-04-19 09:54:02|\"trying to get my...|[{document, 0, 13...|[{language, 0, 13...|\n",
      "|2009-05-03 02:49:30|\" He has and we a...|[{document, 0, 10...|[{language, 0, 10...|\n",
      "|2009-05-17 12:11:13|\"CAYSA canceled t...|[{document, 0, 47...|[{language, 0, 47...|\n",
      "|2009-05-18 04:07:58|\"Worried bout my ...|[{document, 0, 72...|[{language, 0, 72...|\n",
      "|2009-05-18 06:09:44|\"on flight from M...|[{document, 0, 76...|[{language, 0, 76...|\n",
      "|2009-05-22 03:31:21|\",, noo Russia?  ...|[{document, 0, 38...|[{language, 0, 38...|\n",
      "|2009-05-22 04:52:27|\"woke up late cos...|[{document, 0, 47...|[{language, 0, 47...|\n",
      "|2009-05-28 22:07:30|\"AAFFRRIICCAA?!  ...|[{document, 0, 60...|[{language, 0, 60...|\n",
      "|2009-05-29 13:58:34|\"Why are CDs in H...|[{document, 0, 73...|[{language, 0, 73...|\n",
      "|2009-05-30 02:45:30|\" nm  I'm a super...|[{document, 0, 80...|[{language, 0, 80...|\n",
      "|2009-05-30 16:05:53|\" oh *hug* yes i ...|[{document, 0, 90...|[{language, 0, 90...|\n",
      "|2009-05-31 15:16:51|\"Exam week     Ca...|[{document, 0, 43...|[{language, 0, 43...|\n",
      "|2009-05-31 17:48:31|\"Last Fm is down ...|[{document, 0, 67...|[{language, 0, 67...|\n",
      "|2009-05-31 17:54:07|\"Things I love (i...|[{document, 0, 13...|[{language, 0, 13...|\n",
      "|2009-06-01 01:26:28|   \" yes very cold \"|[{document, 0, 16...|[{language, 0, 16...|\n",
      "|2009-06-01 05:03:18|\" no, no exams le...|[{document, 0, 10...|[{language, 0, 10...|\n",
      "|2009-06-01 12:18:13|\"In stupid busine...|[{document, 0, 70...|[{language, 0, 70...|\n",
      "|2009-06-01 13:03:03|\"Woohoo, finished...|[{document, 0, 10...|[{language, 0, 10...|\n",
      "+-------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline = Pipeline(stages=[documentAssembler, langDetector])\n",
    "model = pipeline.fit(sampled_df)\n",
    "sampled_df = model.transform(sampled_df)\n",
    "\n",
    "sampled_df.show()  # This will display the DataFrame with detected languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08a177b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "sampled_df = sampled_df.withColumn(\"detected_language\", col(\"lang.result\").getItem(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "86e7989e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 49:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+-----------------+\n",
      "|              dates|                text|detected_language|\n",
      "+-------------------+--------------------+-----------------+\n",
      "|2009-04-19 08:29:42|\"Twitter API dead...|               en|\n",
      "|2009-05-02 01:14:25|\" My loved ones a...|               en|\n",
      "|2009-05-18 07:41:38|\"Left my coffee a...|               en|\n",
      "|2009-05-30 10:15:33|\"Wishing I could ...|               en|\n",
      "|2009-05-31 07:37:07|\"to day is so unl...|               en|\n",
      "|2009-05-31 15:16:51|\"Exam week     Ca...|               en|\n",
      "|2009-05-31 17:55:07|\"o my! iÂ´m lossi...|               en|\n",
      "|2009-06-01 06:55:39|\"Kind of a rude a...|               en|\n",
      "|2009-06-01 12:18:13|\"In stupid busine...|               en|\n",
      "|2009-06-01 18:28:41|\"just finished ca...|               en|\n",
      "|2009-06-02 02:43:48|\" lol is it sad t...|               en|\n",
      "|2009-06-02 09:50:11|\"Shall I write in...|               en|\n",
      "|2009-06-03 08:14:03|      \"octoglob    \"|               sk|\n",
      "|2009-06-05 00:20:27|\"Is a little upset \"|               en|\n",
      "|2009-06-06 00:19:08|\" oh  whr u headi...|               en|\n",
      "|2009-06-07 02:01:44|\" sad that i can'...|               en|\n",
      "|2009-06-15 09:07:17|\"good morning evr...|               en|\n",
      "|2009-06-16 09:12:08|\"i'm kinda sad an...|               en|\n",
      "|2009-06-16 11:40:25|\" I'm jealous too...|               en|\n",
      "|2009-06-18 01:23:06| \"pusing !! huhh . \"|               fr|\n",
      "+-------------------+--------------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sampled_df = sampled_df.drop(\"document\", \"lang\")\n",
    "sampled_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8bf9496f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows where detected_language is not 'en' from the original result DataFrame\n",
    "sampled_df = sampled_df.filter(F.col(\"detected_language\") == \"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "48408feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 53:=================================================>        (6 + 1) / 7]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered count (English tweets only): 15445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "# Count the total number of observations in the filtered df_eng.\n",
    "en_tweets = sampled_df.count()\n",
    "\n",
    "print(f\"Filtered count (English tweets only): {en_tweets}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6c221d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "\n",
    "# Remove punctuation\n",
    "sampled_df = sampled_df.withColumn('text', regexp_replace(sampled_df['text'], r\"[^\\w\\s]\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f03a83a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lower\n",
    "# Convert to lowercase\n",
    "sampled_df = sampled_df.withColumn('text', lower(sampled_df['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ee42135a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 59:=================================================>        (6 + 1) / 7]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------------------------------+\n",
      "|text                                                                                                                       |\n",
      "+---------------------------------------------------------------------------------------------------------------------------+\n",
      "|  you should come to the mall                                                                                              |\n",
      "| nope never i sooo feel left out now  x                                                                                    |\n",
      "| sure  congrats maybe your luck will rub off on me if i go with you or my bad luck will rub off on you haha                |\n",
      "| moo  i messaged you then tried calling you both numbers my next step is messenger pigeons save me the cost of bread crumbs|\n",
      "| oooh nicei see yr dm but i cant respond on my phone  can i email u real quick                                             |\n",
      "|no baby foxes sighting today                                                                                               |\n",
      "|i got 60 on my sis quiz at fb i lost a chance to have a free sticko                                                        |\n",
      "| temp has dropped a bit here                                                                                               |\n",
      "|  ive totally done the same thing before ie trash your tweet and rewrite  lol  xo                                          |\n",
      "|wants to go to stephs house  blehblehblehbleh                                                                              |\n",
      "| yarn ummprobably anything you love in the recommended weight in the pattern ill go grab some links for ya                 |\n",
      "| lol yeah the is dude at work fixed it for me  everything is up n running                                                  |\n",
      "| its from brooklyn  and etien slavchev  quotesquot                                                                         |\n",
      "|standing in line to get my new phone but i would much rather be watching the today show  i guess thats what youtube is for |\n",
      "|here we go lets make monday pop                                                                                            |\n",
      "| well in the end everyones happy most of the time                                                                          |\n",
      "|in pune for a few hours how can the weather be so much better just 140kms away                                             |\n",
      "| i still aint got me some fries yetbut i will                                                                              |\n",
      "|thats cuz i hardly see you floating around the twitter camp these days  where you beeeee at                                |\n",
      "| s awesome i lurve it                                                                                                      |\n",
      "+---------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sampled_df.select(\"text\").show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "483083c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 63:=================================================>        (6 + 1) / 7]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+-----------------+\n",
      "|              dates|                text|detected_language|\n",
      "+-------------------+--------------------+-----------------+\n",
      "|2009-05-01 21:50:48|going to bedi hav...|               en|\n",
      "|2009-05-04 02:57:10| 158  hadnt train...|               en|\n",
      "|2009-05-14 00:35:57|      both and more |               en|\n",
      "|2009-05-18 06:00:20| the problem weve...|               en|\n",
      "|2009-05-29 23:30:13|i love music vide...|               en|\n",
      "|2009-05-30 02:23:01|rarebreeds farmer...|               en|\n",
      "|2009-05-30 07:45:28|its freakin hott ...|               en|\n",
      "|2009-05-31 14:28:18|i wish i can move...|               en|\n",
      "|2009-05-31 16:39:19|my parents are ba...|               en|\n",
      "|2009-06-01 23:38:16|just watched the ...|               en|\n",
      "+-------------------+--------------------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sampled_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e815bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer\n",
    "\n",
    "# Initialize a regex tokenizer\n",
    "regex_tokenizer = RegexTokenizer(inputCol=\"text\", outputCol=\"tokens\", pattern=\"\\\\W+\")\n",
    "\n",
    "# Transform the dataset\n",
    "tokens = regex_tokenizer.transform(sampled_df)\n",
    "tokens.select(\"tokens\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828f6ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparknlp.annotator import NorvigSweetingModel\n",
    "spell_model = NorvigSweetingModel.pretrained().setInputCols([\"tokens\"]).setOutputCol(\"checked\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6d62dacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparknlp.base import DocumentAssembler\n",
    "\n",
    "document_assembler = DocumentAssembler().setInputCol(\"text\").setOutputCol(\"document\")\n",
    "sampled_df = document_assembler.transform(sampled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "910273d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 70:=================================================>        (6 + 1) / 7]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+-----------------+--------------------+--------------------+\n",
      "|              dates|                text|detected_language|            document|               token|\n",
      "+-------------------+--------------------+-----------------+--------------------+--------------------+\n",
      "|2009-04-07 01:30:32|i want a guy like...|               en|[{document, 0, 35...|[{token, 0, 0, i,...|\n",
      "|2009-04-17 22:06:05|when i saw the tw...|               en|[{document, 0, 12...|[{token, 0, 3, wh...|\n",
      "|2009-05-02 01:14:25| my loved ones ar...|               en|[{document, 0, 30...|[{token, 1, 2, my...|\n",
      "|2009-05-17 12:11:13|caysa canceled th...|               en|[{document, 0, 44...|[{token, 0, 4, ca...|\n",
      "|2009-05-18 06:09:44|on flight from mk...|               en|[{document, 0, 72...|[{token, 0, 1, on...|\n",
      "|2009-05-18 07:41:38|left my coffee at...|               en|[{document, 0, 22...|[{token, 0, 3, le...|\n",
      "|2009-05-28 22:07:30|aaffrriiccaa   i ...|               en|[{document, 0, 55...|[{token, 0, 11, a...|\n",
      "|2009-05-30 02:45:30| nm  im a super g...|               en|[{document, 0, 72...|[{token, 1, 2, nm...|\n",
      "|2009-05-30 10:15:33|wishing i could g...|               en|[{document, 0, 56...|[{token, 0, 6, wi...|\n",
      "|2009-05-31 17:48:31|last fm is down  ...|               en|[{document, 0, 64...|[{token, 0, 3, la...|\n",
      "+-------------------+--------------------+-----------------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from sparknlp.base import DocumentAssembler\n",
    "from sparknlp.annotator import Tokenizer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Initialize DocumentAssembler and Tokenizer\n",
    "document_assembler = DocumentAssembler().setInputCol(\"text\").setOutputCol(\"document\")\n",
    "tokenizer = Tokenizer().setInputCols([\"document\"]).setOutputCol(\"token\")\n",
    "\n",
    "# Create the pipeline with both stages\n",
    "pipeline = Pipeline().setStages([document_assembler, tokenizer])\n",
    "\n",
    "# Fit the pipeline model\n",
    "pipeline_model = pipeline.fit(sampled_df)\n",
    "\n",
    "# Transform the sampled data using the pipeline model\n",
    "sampled_df = pipeline_model.transform(sampled_df)\n",
    "\n",
    "# Show the result\n",
    "sampled_df.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "021d7968",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 77:=================================================>        (6 + 1) / 7]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|token                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[{token, 0, 0, i, {sentence -> 0}, []}, {token, 2, 5, want, {sentence -> 0}, []}, {token, 7, 7, a, {sentence -> 0}, []}, {token, 9, 11, guy, {sentence -> 0}, []}, {token, 13, 16, like, {sentence -> 0}, []}, {token, 18, 25, carmello, {sentence -> 0}, []}, {token, 27, 28, or, {sentence -> 0}, []}, {token, 30, 34, jimmy, {sentence -> 0}, []}]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |\n",
      "|[{token, 2, 8, another, {sentence -> 0}, []}, {token, 10, 12, fun, {sentence -> 0}, []}, {token, 14, 19, confab, {sentence -> 0}, []}, {token, 21, 22, no, {sentence -> 0}, []}, {token, 24, 29, matter, {sentence -> 0}, []}, {token, 31, 34, what, {sentence -> 0}, []}, {token, 36, 40, comes, {sentence -> 0}, []}, {token, 42, 43, of, {sentence -> 0}, []}, {token, 45, 46, it, {sentence -> 0}, []}, {token, 48, 48, i, {sentence -> 0}, []}, {token, 50, 53, wish, {sentence -> 0}, []}, {token, 55, 55, i, {sentence -> 0}, []}, {token, 57, 61, hadnt, {sentence -> 0}, []}, {token, 63, 68, worked, {sentence -> 0}, []}, {token, 70, 74, until, {sentence -> 0}, []}, {token, 76, 76, 9, {sentence -> 0}, []}, {token, 78, 79, im, {sentence -> 0}, []}, {token, 81, 85, gonna, {sentence -> 0}, []}, {token, 87, 90, miss, {sentence -> 0}, []}, {token, 92, 95, next, {sentence -> 0}, []}, {token, 97, 100, week, {sentence -> 0}, []}, {token, 102, 107, though, {sentence -> 0}, []}]                                                                                                                                                                                             |\n",
      "|[{token, 0, 4, going, {sentence -> 0}, []}, {token, 6, 7, to, {sentence -> 0}, []}, {token, 9, 12, bedi, {sentence -> 0}, []}, {token, 14, 17, have, {sentence -> 0}, []}, {token, 19, 22, work, {sentence -> 0}, []}, {token, 24, 31, tomorrow, {sentence -> 0}, []}]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n",
      "|[{token, 1, 2, my, {sentence -> 0}, []}, {token, 4, 8, loved, {sentence -> 0}, []}, {token, 10, 13, ones, {sentence -> 0}, []}, {token, 15, 17, are, {sentence -> 0}, []}, {token, 19, 23, early, {sentence -> 0}, []}, {token, 25, 29, birds, {sentence -> 0}, []}]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |\n",
      "|[{token, 0, 6, sausage, {sentence -> 0}, []}, {token, 8, 14, wrapped, {sentence -> 0}, []}, {token, 16, 19, with, {sentence -> 0}, []}, {token, 21, 25, bacon, {sentence -> 0}, []}, {token, 27, 31, yummy, {sentence -> 0}, []}, {token, 33, 35, but, {sentence -> 0}, []}, {token, 37, 39, not, {sentence -> 0}, []}, {token, 41, 43, too, {sentence -> 0}, []}, {token, 45, 48, good, {sentence -> 0}, []}, {token, 50, 52, for, {sentence -> 0}, []}, {token, 54, 59, throat, {sentence -> 0}, []}]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n",
      "|[{token, 1, 2, he, {sentence -> 0}, []}, {token, 4, 6, has, {sentence -> 0}, []}, {token, 8, 10, and, {sentence -> 0}, []}, {token, 12, 13, we, {sentence -> 0}, []}, {token, 15, 24, appreciate, {sentence -> 0}, []}, {token, 26, 27, it, {sentence -> 0}, []}, {token, 29, 31, but, {sentence -> 0}, []}, {token, 33, 37, danny, {sentence -> 0}, []}, {token, 39, 40, is, {sentence -> 0}, []}, {token, 42, 44, mmm, {sentence -> 0}, []}, {token, 46, 48, mmm, {sentence -> 0}, []}, {token, 50, 53, good, {sentence -> 0}, []}, {token, 55, 55, i, {sentence -> 0}, []}, {token, 57, 61, could, {sentence -> 0}, []}, {token, 63, 64, do, {sentence -> 0}, []}, {token, 66, 72, without, {sentence -> 0}, []}, {token, 74, 76, all, {sentence -> 0}, []}, {token, 78, 79, of, {sentence -> 0}, []}, {token, 81, 83, the, {sentence -> 0}, []}, {token, 85, 91, tattoos, {sentence -> 0}, []}, {token, 93, 98, though, {sentence -> 0}, []}]                                                                                                                                                                                                                                                  |\n",
      "|[{token, 1, 3, 158, {sentence -> 0}, []}, {token, 6, 10, hadnt, {sentence -> 0}, []}, {token, 12, 18, trained, {sentence -> 0}, []}, {token, 20, 23, over, {sentence -> 0}, []}, {token, 25, 27, the, {sentence -> 0}, []}, {token, 29, 32, last, {sentence -> 0}, []}, {token, 34, 34, 2, {sentence -> 0}, []}, {token, 36, 41, months, {sentence -> 0}, []}, {token, 43, 46, came, {sentence -> 0}, []}, {token, 48, 51, last, {sentence -> 0}, []}, {token, 54, 59, slower, {sentence -> 0}, []}, {token, 61, 64, than, {sentence -> 0}, []}, {token, 66, 68, the, {sentence -> 0}, []}, {token, 70, 72, fat, {sentence -> 0}, []}, {token, 74, 76, guy, {sentence -> 0}, []}, {token, 78, 80, and, {sentence -> 0}, []}, {token, 82, 84, old, {sentence -> 0}, []}, {token, 86, 90, woman, {sentence -> 0}, []}, {token, 92, 95, have, {sentence -> 0}, []}, {token, 97, 98, to, {sentence -> 0}, []}, {token, 100, 105, redeem, {sentence -> 0}, []}, {token, 107, 112, myself, {sentence -> 0}, []}, {token, 114, 115, in, {sentence -> 0}, []}, {token, 117, 120, june, {sentence -> 0}, []}, {token, 122, 124, now, {sentence -> 0}, []}]                                                  |\n",
      "|[{token, 0, 3, this, {sentence -> 0}, []}, {token, 5, 7, guy, {sentence -> 0}, []}, {token, 9, 11, has, {sentence -> 0}, []}, {token, 13, 16, lost, {sentence -> 0}, []}, {token, 18, 19, it, {sentence -> 0}, []}]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |\n",
      "|[{token, 0, 1, im, {sentence -> 0}, []}, {token, 3, 10, starving, {sentence -> 0}, []}, {token, 12, 14, and, {sentence -> 0}, []}, {token, 16, 17, my, {sentence -> 0}, []}, {token, 19, 26, teriyaki, {sentence -> 0}, []}, {token, 28, 33, smells, {sentence -> 0}, []}, {token, 35, 36, so, {sentence -> 0}, []}, {token, 38, 43, yumbut, {sentence -> 0}, []}, {token, 45, 46, oh, {sentence -> 0}, []}, {token, 48, 53, please, {sentence -> 0}, []}, {token, 55, 56, 90, {sentence -> 0}, []}, {token, 58, 64, minutes, {sentence -> 0}, []}, {token, 66, 69, left, {sentence -> 0}, []}]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |\n",
      "|[{token, 0, 4, sunny, {sentence -> 0}, []}, {token, 6, 7, is, {sentence -> 0}, []}, {token, 9, 13, soooo, {sentence -> 0}, []}, {token, 15, 18, mean, {sentence -> 0}, []}]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n",
      "|[{token, 0, 3, left, {sentence -> 0}, []}, {token, 5, 6, my, {sentence -> 0}, []}, {token, 8, 13, coffee, {sentence -> 0}, []}, {token, 15, 16, at, {sentence -> 0}, []}, {token, 18, 21, home, {sentence -> 0}, []}]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |\n",
      "|[{token, 0, 3, woke, {sentence -> 0}, []}, {token, 5, 6, up, {sentence -> 0}, []}, {token, 8, 11, late, {sentence -> 0}, []}, {token, 13, 15, cos, {sentence -> 0}, []}, {token, 17, 17, i, {sentence -> 0}, []}, {token, 19, 25, thought, {sentence -> 0}, []}, {token, 27, 31, today, {sentence -> 0}, []}, {token, 33, 35, was, {sentence -> 0}, []}, {token, 37, 44, saturday, {sentence -> 0}, []}]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |\n",
      "|[{token, 0, 8, confusing, {sentence -> 0}, []}]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |\n",
      "|[{token, 0, 6, wishing, {sentence -> 0}, []}, {token, 8, 8, i, {sentence -> 0}, []}, {token, 10, 14, could, {sentence -> 0}, []}, {token, 16, 17, go, {sentence -> 0}, []}, {token, 19, 21, see, {sentence -> 0}, []}, {token, 23, 25, the, {sentence -> 0}, []}, {token, 27, 29, avp, {sentence -> 0}, []}, {token, 31, 35, today, {sentence -> 0}, []}, {token, 38, 40, too, {sentence -> 0}, []}, {token, 42, 45, much, {sentence -> 0}, []}, {token, 47, 50, work, {sentence -> 0}, []}, {token, 52, 53, to, {sentence -> 0}, []}, {token, 55, 56, do, {sentence -> 0}, []}]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |\n",
      "|[{token, 0, 5, woohoo, {sentence -> 0}, []}, {token, 7, 14, finished, {sentence -> 0}, []}, {token, 16, 20, maths, {sentence -> 0}, []}, {token, 22, 28, forever, {sentence -> 0}, []}, {token, 31, 35, thank, {sentence -> 0}, []}, {token, 37, 39, god, {sentence -> 0}, []}, {token, 41, 43, for, {sentence -> 0}, []}, {token, 45, 48, that, {sentence -> 0}, []}, {token, 50, 56, english, {sentence -> 0}, []}, {token, 58, 65, tomorrow, {sentence -> 0}, []}, {token, 67, 75, confident, {sentence -> 0}, []}, {token, 77, 82, really, {sentence -> 0}, []}, {token, 84, 87, miss, {sentence -> 0}, []}, {token, 89, 93, talia, {sentence -> 0}, []}]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |\n",
      "|[{token, 1, 4, west, {sentence -> 0}, []}, {token, 6, 9, cost, {sentence -> 0}, []}, {token, 11, 12, is, {sentence -> 0}, []}, {token, 14, 19, seeing, {sentence -> 0}, []}, {token, 21, 27, project, {sentence -> 0}, []}, {token, 29, 33, natal, {sentence -> 0}, []}, {token, 35, 37, are, {sentence -> 0}, []}, {token, 39, 40, we, {sentence -> 0}, []}, {token, 42, 42, 3, {sentence -> 0}, []}, {token, 44, 48, hours, {sentence -> 0}, []}, {token, 50, 55, behind, {sentence -> 0}, []}, {token, 57, 57, i, {sentence -> 0}, []}, {token, 59, 65, thought, {sentence -> 0}, []}, {token, 67, 68, it, {sentence -> 0}, []}, {token, 70, 72, was, {sentence -> 0}, []}, {token, 74, 77, live, {sentence -> 0}, []}]                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |\n",
      "|[{token, 0, 2, sad, {sentence -> 0}, []}, {token, 4, 8, _____, {sentence -> 0}, []}, {token, 10, 11, is, {sentence -> 0}, []}, {token, 13, 19, leaving, {sentence -> 0}, []}, {token, 21, 24, next, {sentence -> 0}, []}, {token, 26, 29, week, {sentence -> 0}, []}, {token, 32, 35, damn, {sentence -> 0}, []}, {token, 37, 42, summer, {sentence -> 0}, []}, {token, 44, 49, vacays, {sentence -> 0}, []}, {token, 51, 53, but, {sentence -> 0}, []}, {token, 55, 57, ill, {sentence -> 0}, []}, {token, 59, 62, find, {sentence -> 0}, []}, {token, 64, 70, someone, {sentence -> 0}, []}, {token, 72, 75, else, {sentence -> 0}, []}, {token, 77, 78, to, {sentence -> 0}, []}, {token, 80, 88, entertain, {sentence -> 0}, []}, {token, 90, 91, me, {sentence -> 0}, []}, {token, 93, 94, im, {sentence -> 0}, []}, {token, 96, 99, sure, {sentence -> 0}, []}]                                                                                                                                                                                                                                                                                                                              |\n",
      "|[{token, 0, 3, just, {sentence -> 0}, []}, {token, 5, 11, watched, {sentence -> 0}, []}, {token, 13, 15, the, {sentence -> 0}, []}, {token, 17, 19, new, {sentence -> 0}, []}, {token, 21, 23, old, {sentence -> 0}, []}, {token, 25, 32, republic, {sentence -> 0}, []}, {token, 34, 36, mmo, {sentence -> 0}, []}, {token, 38, 44, trailer, {sentence -> 0}, []}, {token, 46, 49, from, {sentence -> 0}, []}, {token, 51, 52, e3, {sentence -> 0}, []}, {token, 55, 61, bioware, {sentence -> 0}, []}, {token, 63, 65, can, {sentence -> 0}, []}, {token, 67, 70, make, {sentence -> 0}, []}, {token, 72, 72, a, {sentence -> 0}, []}, {token, 74, 78, video, {sentence -> 0}, []}, {token, 81, 81, i, {sentence -> 0}, []}, {token, 83, 88, really, {sentence -> 0}, []}, {token, 90, 93, wish, {sentence -> 0}, []}, {token, 95, 96, it, {sentence -> 0}, []}, {token, 98, 100, was, {sentence -> 0}, []}, {token, 102, 102, a, {sentence -> 0}, []}, {token, 104, 109, single, {sentence -> 0}, []}, {token, 111, 116, player, {sentence -> 0}, []}, {token, 118, 122, kotor, {sentence -> 0}, []}, {token, 124, 124, 3, {sentence -> 0}, []}, {token, 126, 131, though, {sentence -> 0}, []}]|\n",
      "|[{token, 0, 6, traffic, {sentence -> 0}, []}, {token, 8, 13, lights, {sentence -> 0}, []}, {token, 15, 16, at, {sentence -> 0}, []}, {token, 18, 24, windsor, {sentence -> 0}, []}, {token, 26, 31, bridge, {sentence -> 0}, []}, {token, 33, 35, are, {sentence -> 0}, []}, {token, 37, 40, only, {sentence -> 0}, []}, {token, 42, 48, letting, {sentence -> 0}, []}, {token, 50, 56, through, {sentence -> 0}, []}, {token, 58, 58, 3, {sentence -> 0}, []}, {token, 60, 67, vehicles, {sentence -> 0}, []}, {token, 69, 70, at, {sentence -> 0}, []}, {token, 72, 72, a, {sentence -> 0}, []}, {token, 74, 77, time, {sentence -> 0}, []}, {token, 80, 86, causing, {sentence -> 0}, []}, {token, 88, 96, tailbacks, {sentence -> 0}, []}, {token, 98, 99, on, {sentence -> 0}, []}, {token, 101, 105, upper, {sentence -> 0}, []}, {token, 107, 113, bristol, {sentence -> 0}, []}, {token, 115, 116, rd, {sentence -> 0}, []}, {token, 118, 120, amp, {sentence -> 0}, []}, {token, 122, 125, late, {sentence -> 0}, []}, {token, 127, 131, buses, {sentence -> 0}, []}]                                                                                                                     |\n",
      "|[{token, 0, 0, i, {sentence -> 0}, []}, {token, 2, 7, really, {sentence -> 0}, []}, {token, 9, 12, wana, {sentence -> 0}, []}, {token, 14, 15, go, {sentence -> 0}, []}, {token, 17, 18, on, {sentence -> 0}, []}, {token, 20, 26, holiday, {sentence -> 0}, []}, {token, 29, 35, someone, {sentence -> 0}, []}, {token, 37, 40, take, {sentence -> 0}, []}, {token, 42, 43, me, {sentence -> 0}, []}, {token, 45, 46, to, {sentence -> 0}, []}, {token, 48, 52, italy, {sentence -> 0}, []}]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sampled_df.select(\"token\").show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d27838b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spell_model = NorvigSweetingModel.pretrained().setInputCols([\"tokens\"]).setOutputCol(\"checked\")\n",
    "\n",
    "# Transform the tokenized dataset\n",
    "sampled_df = spell_model.transform(tokens)\n",
    "sampled_df.select(\"checked\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1074257",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f4d401",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "# Initialize a stopwords remover\n",
    "remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered_tokens\")\n",
    "\n",
    "# Transform the tokenized data\n",
    "sampled_df = remover.transform(tokens)\n",
    "sampled_df.select(\"filtered_tokens\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92544293",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
